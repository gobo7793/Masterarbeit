\section{Problemstellung}\label{sec:problemstellung}

Am Softwaretchnik-Lehrstuhl der Universität Augsburg wurde in den letzten Jahren das Framework \sS (sprich \emph{Safety Sharp}) entwickelt. Dieses Framework kann dazu genutzt werden, selbstorganisierte und sicherheitskritische Systeme zu analysieren und ihre Schwächen zu ermitteln \cite{Habermaier2016}\cite{Habermaier2015}. Dazu wurde auch bereits die ZNN.com-Fallstudie, welche Shang-Wen Cheng in seiner Dissertation beschrieben hat \cite{Cheng2008}, als vereinfachtes Modell mit den wesentlichen Bestandteilen implementiert. Nun soll nicht mehr nur dieses Modell getestet werden, sondern ein reales System mit einem ähnlichen Aufbau. Dafür wurde Apache Hadoop\footnote{\url{https://hadoop.apache.org/}} ausgewählt, welches in der Industrie im Bereich Datenverarbeitung und dem Stichwort \emph{Big Data} eingesetzt wird. Mit Hadoop ist es möglich, ein Servercluster zu erstellen, auf dem anschließend dafür entwickelte Anwendungen ausgeführt werden. Es soll daher nun getestet und analysiert werden, wie sich Hadoop unter verschiedenen Lastprofilen verhält und dabei bestimmte Fehler auftreten, wie z. B. wenn die Verbindung zu einem der Server verloren geht oder ein Server komplett ausfällt.

Bei der ZNN.com-Fallstudie als reines Modell gab es bereits eine ähnliche Aufgabenstellung, welche im Positionspapier \cite{Eberhardinger2017} genauer erläutert wurde. Das Hauptziel dieses Projektes ist daher nun, anstatt eines reinen Modells ein reales System zu testen. Dafür wird ein modellbasierter Ansatz als Testkonzept genutzt und Hadoop zunächst als Modell nachgebildet. Dieses Modell wird dann dazu genutzt, um ein reales Hadoop-Cluster entsprechend zu steuern und zu testen, wie sich das reale System unter bestimmten Bedingungen verhält und dabei zu ermitteln, wann es nicht mehr funktionsfähig ist.

\subsection{Aufbau des Modells}\label{sec:modellaufbau}

Zunächst muss natürlich erst einmal der Versuchsaufbau selbst in \sS modelliert werden. Ein Modell beinhaltet in \sS zunächst einmal die Komponenten des Systems und deren Zusammenhänge, also wie die Komponenten miteinander agieren. Wichtig sind in einem \sS-Modell aber auch mögliche Komponentenfehler, welche bekannt sind und jederzeit auftreten können. Komponentenfehler werden bereits in der Designphase eines Modells eingearbeitet und können bei der späteren Ausführung flexibel aktiviert und deaktiviert werden, um die Probleme des zu testenden Systems zu ermitteln (vgl. \autoref{sec:sSharp}).

Um nun Hadoop in \sS zu modellieren wird zunächst ein Konzept erstellt, in dem ausgearbeitet wird, welche Komponenten und Komponentenfehler relevant sind. Anschließend müssen deren Zusammenhänge und wesentlichen Eigenschaften ausgearbeitet werden und in das Konzept übernommen werden. Sobald das Konzept fertig ausgearbeitet ist, kann das Modell in \sS implementiert werden.

\subsection{Mapping zum realen System}\label{sec:mapping}

Nachdem die Basis des Modells steht, kann die Funktionalität entwickelt werden. Dazu werden in \sS nur Basisfunktionen eingebaut, um mit dem realen System kommunizieren zu können. Dies geschieht mit einer Art Treiber, welcher mithilfe von mehreren SSH-Verbindungen mit dem realen Hadoop-Cluster kommuniziert und so das Mapping zwischen Modell und realem System übernimmt. Jede der SSH-Verbindungen ist dabei nur für einen Einsatzzweck gedacht, sodass es Verbindungen für \uA folgende Einsatzzwecke gibt:

\begin{itemize}[noitemsep]
	\item Starten von Benchmark-Anwendungen
	\item Monitoring des realen Cluster
	\item Injektion von Komponentenfehler
\end{itemize}

Der Vorteil von mehreren Verbindungen liegt darin, dass jede Verbindung unabhängig ist und nicht auf die Antwort des zuvor gestarteten Programms warten muss. So ist es möglich, mithilfe mehrere Verbindungen mehrere Anwendungen parallel zu starten und jede Rückgabe auszuwerten und währenddessen verschiedene Komponentenfehler zu aktivieren.

\subsection{Erstellung der Lastprofile}\label{sec:lastprofilerstellung}

Sobald das Grundmodell steht, können die Testfälle selbst entwickelt werden. Als Testfälle dienen dazu unterschiedliche Lastprofile, um verschiedene Auslastungsgrade und Nutzungsszenarien zu simulieren. Dazu sollen die Lastprofile verschiedene Benchmarks beinhalten, deren einzelne Anwendungen kombiniert oder alleine auf dem realen System ausgeführt werden:

\begin{itemize}[noitemsep]
	\item Hadoop Mapreduce Examples
	\item Intel HiBench
	\item SWIM (Statistical Workload Injector for Mapreduce)
\end{itemize}

Eine Besonderheit bildet der SWIM-Benchmark, welcher sehr Ressourcenintensiv ist und daher auf einem \emph{Single Node Cluster} sehr Zeitintensiv sein kann. Der Intel HiBench basiert auf einzelnen Bestandteilen der Mapreduce Examples. Die Examples wiederum sind nicht mehr als zahlreiche voneinander unabhängige Beispielanwendungen für Hadoop, wodurch die Möglichkeit besteht, sehr einfach unterschiedliche Profile zu simulieren, je nachdem welche der Einzelanwendungen alleine oder parallel gestartet werden. Daher muss zunächst auch geprüft werden, welcher Benchmark welche Möglichkeiten bietet, um die benötigten Testfälle bzw. Lastprofile zu erstellen und so den dynamischen Teil des zu testenden Modells zu erstellen.

\subsection{Erstellen und Ausführen der Tests}\label{sec:testausführung}

Sobald Modell und Testfälle stehen, kann mit der Erstellung der Tests fortgefahren werden. Die Tests müssen nun so erstellt werden, dass sie sich einerseits auf veränderte Bedingungen des realen Clusters anpassen, aber auch automatisiert die einzelnen Anwendungen der Lastprofile aktivieren und ausführen. Dies schließt auch unterschiedliche Profile für die Aktivierung der Komponentenfehler ein. Zum einen kann nur eine Simulation ohne Fehler gestartet werden, zum anderen aber auch unterschiedliche Komponentenfehler aktiviert werden. Der \emph{Model Checker} von \sS besitzt dazu auch Möglichkeiten, um Komponentenfehler auch kombiniert auszuführen. Dazu werden basierend auf zuvor definierte \emph{Constraints} Komponentenfehler aktiviert, um so typische Probleme des realen Systems zu simulieren. Basierend darauf wird dann ermittelt, welche Fehler nun im realen System auftauchen.

\subsection{Evaluierung der Ergebnisse}\label{sec:evaluierung}

Je nachdem welche Constraints bei der Ausführung genutzt werden, sind nun unterschiedliche Fehler und Daten im realen System ermittelt worden, welche zum Abschluss evaluiert werden müssen. Einige Erwartungen sind da natürlich bereits im Voraus klar: Sollte es zu einem Netzwerk- oder Serverausfall eines Hadoop-Nodes kommen, muss das System dies selbstständig erkennen und die Anwendung an einen anderen Node abgeben. Dabei sollte das System auch erkennen, welche anderen Nodes bereits beschäftigt sind und entsprechend auf dem von Hadoop genutzten \emph{Load Balancer} einen Node auswählen. Neben einer Fehleranalyse können aber auch die Laufzeiten unter bestimmten Bedingungen analysiert werden.