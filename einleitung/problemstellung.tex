\section{Problemstellung}\label{sec:problemstellung}

Nun gibt es zahlreiche MC, wie \zB \emph{LTSmin}\footnote{\url{http://fmt.cs.utwente.nl/tools/ltsmin/}}. Auch am Institut für Software \& Systems Engineering der Universität Augsburg wurde in den letzten Jahren mit \sS (sprich \emph{Safety Sharp}) ein entsprechendes Framework zum testen von sicherheitskritischen und selbst-adaptiven Systemen basierend auf dem MC-Ansatz entwickelt \cite{Habermaier2015,Habermaier2016}. Nun ist das aktuelle Vorhaben, mithilfe von \sS ein reales Serversystem zu testen, welches bereits als theoretisches Modell basierend auf der ZNN.vom-Fallstudie, bekannt aus der Dissertation von Shang-Wen Cheng \cite{Cheng2008}, implementiert wurde. Als reales System soll nun \emph{Apache Hadoop}\footnote{\url{https://hadoop.apache.org/}} getestet werden, welches in der Industrie im Bereich Datenverarbeitung eingesetzt wird. Mit Hadoop ist es möglich, ein Servercluster zu erstellen, auf dem anschließend dafür entwickelte Anwendungen ausgeführt werden und somit große Datenmengen zu verarbeiten. Es soll daher nun getestet und analysiert werden, wie sich Hadoop unter verschiedenen Lastprofilen verhält und dabei bestimmte Fehler auftreten, wenn z. B. einer der Hadoop-Nodes ausfällt.

Bei der ZNN.com-Fallstudie als reines Modell gab es bereits eine ähnliche Aufgabenstellung, welche im Positionspapier \cite{Eberhardinger2017} genauer erläutert wurde. Das Hauptziel dieses Projektes ist daher nun, anstatt eines reinen Modells ein reales System zu testen. Dafür wird ein modellbasierter Ansatz als Testkonzept genutzt und Hadoop zunächst als Modell nachgebildet. Dieses Modell wird dann dazu genutzt, um ein reales Hadoop-Cluster entsprechend zu steuern und mithilfe des MC-Ansatzes zu testen, wie sich das reale System unter bestimmten Bedingungen verhält und dabei zu ermitteln, wann es nicht mehr funktionsfähig ist.

\subsection{Aufbau des Modells}\label{sec:modellaufbau}

Zunächst muss natürlich erst einmal der Versuchsaufbau selbst in \sS modelliert werden. Ein Modell beinhaltet in \sS zunächst einmal die Komponenten des Systems und deren Zusammenhänge, also wie die Komponenten miteinander agieren. Wichtig sind in einem \sS-Modell aber auch mögliche Komponentenfehler, welche bekannt sind und jederzeit auftreten können. Komponentenfehler werden bereits in der Designphase eines Modells eingearbeitet und können bei der späteren Ausführung flexibel aktiviert und deaktiviert werden, um die Probleme des zu testenden Systems zu ermitteln (vgl. \autoref{sec:sSharp}).

Um nun Hadoop in \sS zu modellieren wird zunächst ein Konzept erstellt, in dem ausgearbeitet wird, welche Komponenten und Komponentenfehler relevant sind. Anschließend müssen deren Zusammenhänge und wesentlichen Eigenschaften ausgearbeitet werden und in das Konzept übernommen werden. Sobald das Konzept fertig ausgearbeitet ist, kann das Modell in \sS implementiert werden.

\subsection{Mapping zum realen System}\label{sec:mapping}

Nachdem die Basis des Modells steht, kann die Funktionalität entwickelt werden. Dazu werden in \sS nur Basisfunktionen eingebaut, um mit dem realen System kommunizieren zu können. Um für das reale System möglichst viele Ressourcen zur Verfügung zu stellen, wird das reale System auf einem eigenen PC installiert. Die Verbindung zwischen Modell und realem System wird mit einer Art Treiber realisiert, welcher mithilfe von mehreren SSH-Verbindungen mit dem realen Hadoop-Cluster kommuniziert und so das Mapping zwischen Modell und realem System übernimmt. Jede der SSH-Verbindungen ist dabei nur für einen Einsatzzweck gedacht, sodass es Verbindungen für \uA folgende Einsatzzwecke gibt:

\begin{itemize}[noitemsep]
	\item Starten von Benchmark-Anwendungen
	\item Monitoring des realen Cluster
	\item Injektion von Komponentenfehler
\end{itemize}

Der Vorteil von mehreren Verbindungen liegt darin, dass jede Verbindung unabhängig ist und nicht auf die Antwort des zuvor ausgeführten Befehls einer anderen Verbindung warten muss. So ist es möglich, mithilfe mehrerer Verbindungen mehrere Anwendungen parallel zu starten und jede Rückgabe auszuwerten und währenddessen verschiedene Komponentenfehler zu aktivieren.

\subsection{Erstellung der Lastprofile}\label{sec:lastprofilerstellung}

Sobald das Grundmodell steht, können die Testfälle selbst entwickelt werden. Als Testfälle dienen dazu unterschiedliche Lastprofile, um verschiedene Auslastungsgrade und Nutzungsszenarien zu simulieren. Dazu sollen die Lastprofile verschiedene Benchmarks beinhalten, deren einzelne Anwendungen kombiniert oder alleine auf dem realen System ausgeführt werden:

\begin{itemize}[noitemsep]
	\item Hadoop Mapreduce Examples
	\item Intel HiBench
	\item SWIM (Statistical Workload Injector for Mapreduce)
\end{itemize}

Eine Besonderheit bildet der SWIM-Benchmark, welcher sehr Ressourcenintensiv ist und daher auf einem \emph{Single Node Cluster}, also einem kompletten Hadoop-Cluster auf nur einem Computer, sehr zeitintensiv sein kann. Der Intel HiBench basiert auf einzelnen Bestandteilen der Mapreduce Examples. Die Examples wiederum sind zahlreiche voneinander unabhängige Beispielanwendungen für Hadoop. Dadurch besteht die Möglichkeit, abhängig davon, welche Anwendungen einzeln oder parallel gestartet werden,  unterschiedliche Profile zu simulieren. Daher muss zunächst auch geprüft werden, welcher Benchmark welche Möglichkeiten bietet, um die benötigten Testfälle bzw. Lastprofile zu erstellen und so den dynamischen Teil des zu testenden Modells zu erstellen.

\subsection{Erstellen und Ausführen der Tests}\label{sec:testausführung}

Sobald Modell und Testfälle stehen, kann mit der Erstellung der Tests fortgefahren werden. Die Tests müssen nun so erstellt werden, dass sie sich einerseits auf veränderte Bedingungen des realen Clusters anpassen, aber auch automatisiert die einzelnen Anwendungen der Lastprofile aktivieren und ausführen. Dies schließt auch unterschiedliche Profile für die Aktivierung der Komponentenfehler ein. Zum einen kann nur eine Simulation ohne Fehler gestartet werden, zum anderen aber auch unterschiedliche Komponentenfehler aktiviert werden. Der MC von \sS besitzt dazu auch Möglichkeiten, um Komponentenfehler kombiniert auszuführen. Dazu werden basierend auf zuvor definierte \emph{Constraints} Komponentenfehler aktiviert, um so typische Probleme des realen Systems zu simulieren. Basierend darauf wird dann ermittelt, welche Fehler nun im realen System auftauchen.

\subsection{Evaluierung der Ergebnisse}\label{sec:evaluierung}

Je nachdem welche \emph{Constraints} bei der Ausführung genutzt werden, sind nun unterschiedliche Fehler und Daten im realen System ermittelt worden, welche zum Abschluss evaluiert werden müssen. Einige Erwartungen sind da natürlich bereits im Voraus klar: Sollte es zu einem Netzwerk- oder Serverausfall eines Hadoop-Nodes kommen, muss das System dies selbstständig erkennen und die Anwendung an einen anderen Node abgeben. Dabei sollte das System auch erkennen, welche anderen Nodes bereits beschäftigt sind und entsprechend auf dem von Hadoop genutzten \emph{Load Balancer} einen Node auswählen. Neben einer Fehleranalyse können aber auch die Laufzeiten unter bestimmten Bedingungen analysiert werden.
