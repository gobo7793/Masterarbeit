\section{Anforderungen an das Testsystem}
\label{sec:evaluationPlan}

Um das Testsystem zu validieren, wurde zunächst ein Evaluationsplan aufgestellt.
In diesem ist festgehalten, was getestet wird, wie die Testfälle aussehen und wie die bei der Ausführung gewonnen Daten organisiert werden.

\subsection{Behauptungen und Variablen}
\label{sec:predictions}

Neben den in \autoref{sec:clusterRequirements} genannten funktionalen Anforderungen, bestehen an das gesamte Testsystem weitere Anforderungen.
Zur Evaluation des Testsystems wurden hierbei folgende, weiteren Anforderungen bzw. Behauptungen aufgestellt:

\begin{enumerate}
    \setcounter{enumi}{4}
    \item Der jeweils aktuelle Status des Clusters wird erkannt und im Modell gespeichert
    \item Defekte Nodes und Verbindungsabbrüche werden erkannt
    \item Im Modell implementierte Komponentenfehler werden im realen Cluster injiziert und repariert
    \item Wenn alle Nodes defekt sind, wird erkannt, dass sich das Cluster nicht mehr rekonfigurieren kann
    \item Ein Test kann vollautomatisch ausgeführt werden
    \item Das Cluster kann ohne Auswirkungen auf die Funktionsweise auf einem oder beiden Hosts ausgeführt werden
    \item Es können mehrere Benchmark"=Anwendungen gleichzeitig gestartet und ausgeführt werden
    \item Ein Testfall kann zeitlich unabhängig und mehrmals ausgeführt werden
\end{enumerate}

Möglichst viele Anforderungen sollen, sofern möglich, im Testsystem implementiert werden, sodass sie in Form von \emph{Constraints} während der Ausführung der Tests bereits automatisch validiert werden können (vgl. \todo{abschnitt constraints impl}).

Basierend auf den Anforderungen wurden folgende unabhängigen Variablen zur Evaluation ermittelt:

\begin{itemize}
    \item Anzahl der Hosts und Nodes
    \item Anzahl der Clients
    \item Anzahl der Simulations"=Schritte
    \item \emph{Seed} für Zufallsgeneratoren
    \item Generelle Wahrscheinlichkeit zur Aktivierung und Deaktivierung der Komponentenfehler
\end{itemize}

Basierend auf den unabhängigen Variablen wurden folgende abhängigen Variablen ermittelt:

\begin{itemize}
    \item Art und Anzahl der aktivierten und deaktivierten Komponentenfehler
    \item Anzahl erfolgreich bzw. nicht erfolgreich beendeter Anwendungen
    \item Anzahl abgebrochener Anwendungen
\end{itemize}

Diese Variablen werden dazu genutzt, um einerseits einzelne Testfälle zu definieren, andererseits auch als primäre Kennzahlen im Rahmen der Evaluation in \autoref{chap:evaluation}.

\subsection{Generierung der Testfälle}
\label{sec:testcaseGeneration}

Die Testfälle werden anhand der im vorherigen \autoref{sec:predictions} genannten unabhängigen Variablen definiert.
Ein Testfall ist somit zum einen Abhängig von der Größe des Clusters, also ob das Cluster auf einem oder beiden Hosts ausgeführt wird und aus wie vielen Nodes das Cluster besteht.
Mithilfe des Seeds für die Zufallsgeneratoren werden die ausgeführten Anwendungen anhand des Transitionssystems ermittelt.
Die Anzahl der insgesamt ausgeführten Anwendungen wird primär von der Anzahl der Simulations"=Schritte sowie der Anzahl der Clients bestimmt.
Die zu injizierenden und zu reparierenden Komponentenfehler, mit denen defekte Nodes und Verbindungsausfälle simuliert werden, werden zum Teil mithilfe des ausgewählt.
Zudem sollen neben den Tests mit einem normalen Cluster zusätzliche Mutationstest mit einer veränderten Selfbalancing"=Komponente ausgeführt werden.

Der in dieser Fallstudie verwendete Zufallsgenerator des .NET"=Frameworks ist hierbei kein \emph{echter} Zufallsgenerator, sondern ein Pseudo"=Zufallsgenerator, der mithilfe mathematischer Formeln anhand eines Start"=Seeds Zufallszahlen ermittelt, die anschließend zur Auswahl der Anwendungen und Komponentenfehlern genutzt werden.
Da standardmäßig ein zeitbasierter Seed zur Initialisierung des Zufallsgenerators genutzt wird, muss zur Wiederausführbarkeit einzelner Testfälle der Seed der Zufallsgeneratoren manuell festgelegt werden können.
\todo{Wo wird Seed wie genutzt? am besten in implementierung davon!}

Die Auswahl der konkreten Testfälle ist in \autoref{sec:selectTestcases} beschrieben, die Details zu deren Implementierung in \autoref{sec:implTestcases}.

\subsection{Organisation und Ausgabe der Daten}
\label{sec:dataOrganisation}

Damit die bei der Ausführung gewonnenen Daten auch zur Evaluation genutzt werden können, wurde hierzu festgelegt, welche Daten während der Ausführung ausgegeben werden.
Alle relevante Daten werden hierzu während der Ausführung der Testfälle in einer Log"=Datei gespeichert.
Zur Unterscheidung von einzelnen Ausführungen werden die Daten klar strukturiert.

Beim Start eines Testfalls sollen daher zunächst einige generelle Daten ausgegeben werden:

\begin{itemize}
    \item Basis"=Seed für die Zufallsgeneratoren
    \item Wahrscheinlichkeit für Aktivierung und Deaktivierung der Komponentenfehler
    \item Anzahl genutzter Hosts, Nodes und Clients
    \item Anzahl der ausgeführten Simulations"=Schritte
    \item Angabe, ob ein normaler Test oder ein Mutationstest ausgeführt wird
\end{itemize}

Im Rahmen der Simulation können weitere Daten ausgegeben werden, wie \zB:

\begin{itemize}
    \item Angabe, ob vorab generierte Eingabedaten genutzt werden oder diese während der Ausführung eines Testfalls generiert werden
    \item Mindestdauer für einen Simulations"=Schritt
    \item Auszuführende Benchmarks pro Client
\end{itemize}

Die Ausgabe der Daten der YARN"=Komponenten wird bei jedem Simulations"=Schritt durchgeführt, damit das Verhalten des Clusters berücksichtigt werden kann.
Ausgegeben werde hierbei:

\begin{itemize}
    \item Für jeden Node:
    \begin{itemize}
        \item ID bzw. Name des Nodes
        \item Aktueller Status
        \item Informationen zur Fehleraktivierung
        \item Anzahl ausgeführter Container auf dem Node
        \item Angaben zur Speicherauslastung
        \item Angaben zur CPU"=Auslastung
    \end{itemize}
    
    \item Für jeden Client:
    \begin{itemize}
        \item ID bzw. Name des Clints
        \item Aktuell ausgeführter Benchmark
        \item ID der aktuell ausgeführten Anwendung auf dem Cluster
    \end{itemize}

    \item Für jede Anwendung:
    \begin{itemize}
        \item ID der Anwendung
        \item Bezeichnung der Anwendung
        \item Aktueller und finaler Status der Anwendung
        \item ID bzw. Name des Nodes, auf dem der \ac{AppMstr} ausgeführt wird
    \end{itemize}

    \item Für jeden Attempt:
    \begin{itemize}
        \item ID des Attempts
        \item Aktueller Status des Attempts
        \item ID des \ac{AppMstr}"=Containers
        \item ID bzw. Name des Nodes, auf dem der \ac{AppMstr} ausgeführt wird
    \end{itemize}

    \item Für jeden Container:
    \begin{itemize}
        \item ID des Containers
        \item ID bzw. Name des auszuführenden Nodes
        \item Aktueller Status des Containers
    \end{itemize}
\end{itemize}

Die Details zur Implementierung und dem Ausgabeformat während der Ausführung der Simulation sind in \autoref{sec:simulationStepOutput} erläutert, die zur Ausgabe der generellen Testfalldaten in \autoref{sec:implTestcases}. Ein Beispiel einer möglichen Ausgabe für einen Testfall findet sich in \autoref{app:outputFormat}.
