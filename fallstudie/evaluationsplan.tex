\section{Planung der Tests und der Evaluation}
\label{sec:evaluationPlan}

Um das Testsystem zu validieren, wurde zunächst ein Evaluationsplan aufgestellt.
In diesem ist festgehalten, was getestet wird, wie die Testkonfigurationen definiert sind, und wie die bei der Ausführung gewonnen Daten organisiert werden.

\subsection{Behauptungen und Variablen}
\label{subsec:theses}

Als Basis für die Behauptungen und Variablen dienten die in \autoref{sec:requirements} definierten Anforderungen an das Cluster und das Testsystem.
Sie gehen damit auch einher mit den Behauptungen, welche für die Evaluation aufgestellt werden.
Basierend auf den Anforderungen wurden daher folgende unabhängigen Variablen zur Evaluation ermittelt:

\begin{itemize}
    \item Anzahl der Hosts und Nodes
    \item Anzahl der Clients
    \item Anzahl der Testfälle pro Test
    \item \emph{Seed} für Zufallsgeneratoren
    \item Generelle Wahrscheinlichkeit zur Aktivierung und Deaktivierung der Komponentenfehler
\end{itemize}

Basierend auf den unabhängigen Variablen wurden \uA folgende abhängigen Variablen ermittelt:

\begin{itemize}
    \item Anzahl der tatsächlich ausgeführten Testfälle
    \item Aktivierten und deaktivierte Komponentenfehler
    \item Anzahl ausgeführter Anwendungen
    \item Anzahl und Gründe für evtl. nicht vollständig ausgeführte Anwendungen
\end{itemize}

Während die unabhängigen Variablen dazu genutzt werden, um Testkonfigurationen zu definieren (vgl. \autoref{subsec:testcaseGeneration}), dienen die abhängigen Variablen als wichtige Kennzahlen im Rahmen der Evaluation in \autoref{ch:evaluationResults}.

\subsection{Generierung der Testkonfigurationen}
\label{subsec:testcaseGeneration}

Um nun anhand der in \autoref{subsec:theses} definierten Variablen die in \autoref{sec:requirements} definierten Anforderungen zu prüfen, sind mehrere Tests nötig.
Als Basis zur Definition einer Testkonfiguration dienen die in \autoref{subsec:theses} definierten unabhängigen Variablen, die durch weitere Angaben ergänzt werden:

\begin{itemize}
    \item Anzahl genutzter Hosts
    \item Basisanzahl der Nodes
    \item Anzahl simulierter Clients
    \item Verwendeter Seed für Zufallsgeneratoren
    \item Anzahl auszuführender Testfälle
    \item Mindestdauer für einen Testfall
    \item Nutzung einer oder mehreren Mutationen
    \item Generelle Wahrscheinlichkeit zur Aktivierung von Komponentenfehlern
    \item Generelle Wahrscheinlichkeit zur Deaktivierung von Komponentenfehlern
    \item Verwendung von vorab generierten Eingabedaten
\end{itemize}

Die Auswahl der ausgeführten Anwendungen erfolgt während der Testausführung anhand des Seeds der Testkonfiguration.
Zwar werden durch das Transitionssystem wahrscheinlichkeitsbasierend zufällig Anwendungen ausgewählt, jedoch kann dies durch den Charakter des im .NET"=Framework vorhandenen Zufallsgenerators gesteuert werden.
Der in .NET implementierte Zufallsgenerator ist nämlich kein \emph{echter}, sondern ein Pseudo"=Zufallsgenerator, der mithilfe von mathematischen Formeln und anhand eines Seeds Zufallszahlen berechnet.
Zwar wird standardmäßig ein zeitbasierter Seed zur Initialisierung des Zufallsgenerators genutzt, durch die Angabe eines spezifischen Seeds kann jedoch die Wiederausführbarkeit der Tests sichergestellt werden.
\todo{Wo wird Seed wie genutzt? am besten in implementierung davon!}

Auch die Aktivierung und Deaktivierung von Komponentenfehlern selbst soll während der Ausführung eines Testfalls festgelegt werden, wodurch die Wahrscheinlichkeit für deren Aktivierung bzw. Deaktivierung einen maßgeblichen Einfluss besitzt.
Da Anwendungen, die Eingabedaten für andere Anwendungen generieren, \uU nicht erfolgreich beendet werden können, kann eine Testkonfiguration mit vorab generierten und im \ac{HDFS} gespeicherten Daten durchgeführt werden.
Dadurch wird es ermöglicht, dass in solchen Tests spätere Anwendungen nicht vom Erfolg der zuvor ausgeführten Anwendungen zur Generierung der Eingabedaten abhängig sind.
Da einige Tests zudem als Mutationstests durchgeführt werden sollen, muss für eine Testkonfiguration dies entsprechend definiert werden.

Die Auswahl der konkreten Testkonfigurationen in \autoref{sec:selectTestcases} erfolgte im Rahmen der in \autoref{sec:implTestcases} beschriebenen Implementierung der Tests.

\subsection{Organisation und Ausgabe der Daten}
\label{subsec:dataOrganisation}

Damit die bei der Ausführung gewonnenen Daten auch zur Evaluation genutzt werden können, wurde hierzu festgelegt, welche Daten während der Ausführung ausgegeben werden.
Alle relevante Daten werden hierzu während der Ausführung der Testfälle in eine Log"=Datei ausgegeben und gespeichert.
Zur Unterscheidung von einzelnen Ausführungen werden die Daten klar strukturiert.

Beim Start eines Testfalls sollen daher zunächst einige generelle Daten ausgegeben werden:

\begin{itemize}
    \item Basis"=Seed für die Zufallsgeneratoren
    \item Wahrscheinlichkeit für Aktivierung und Deaktivierung der Komponentenfehler
    \item Anzahl genutzter Hosts, Nodes und Clients
    \item Anzahl der ausgeführten Simulations"=Schritte
    \item Angabe, ob ein normaler Test oder ein Mutationstest ausgeführt wird
\end{itemize}

Im Rahmen der Simulation können weitere Daten ausgegeben werden, wie \zB:

\begin{itemize}
    \item Angabe, ob vorab generierte Eingabedaten genutzt werden oder diese während der Ausführung eines Testfalls generiert werden
    \item Mindestdauer für einen Simulations"=Schritt
    \item Auszuführende Benchmarks pro Client
\end{itemize}

Die Ausgabe der Daten der \ac{YARN}"=Komponenten wird bei jedem erfolgreichen Testfall durchgeführt, damit das Verhalten des Clusters berücksichtigt werden kann.
Bei nicht erfolgreichen Testfällen wird die Simulation dagegen beendet.
Für solche Fälle werden nach Abschluss der Simulation erneut die Daten des Clusters ausgelesen und ausgegeben.
Es können hierbei alle Daten ausgegeben werden, welche erkannt werden können, mindestens jedoch:

\begin{itemize}
    \item Für jeden Node:
    \begin{itemize}
        \item ID bzw. Name des Nodes
        \item Aktueller Status
        \item Informationen zur Fehleraktivierung
        \item Anzahl ausgeführter Container auf dem Node
        \item Angaben zur Speicherauslastung
        \item Angaben zur CPU"=Auslastung
    \end{itemize}
    
    \item Für jeden Client:
    \begin{itemize}
        \item ID bzw. Name des Clints
        \item Aktuell ausgeführter Benchmark
        \item ID der aktuell ausgeführten Anwendung auf dem Cluster
    \end{itemize}

    \item Für jede Anwendung:
    \begin{itemize}
        \item ID der Anwendung
        \item Bezeichnung der Anwendung
        \item Aktueller und finaler Status der Anwendung
        \item ID bzw. Name des Nodes, auf dem der \ac{AppMstr} ausgeführt wird
    \end{itemize}

    \item Für jeden Attempt:
    \begin{itemize}
        \item ID des Attempts
        \item Aktueller Status des Attempts
        \item ID des \ac{AppMstr}"=Containers
        \item ID bzw. Name des Nodes, auf dem der \ac{AppMstr} ausgeführt wird
        \item Anzahl der derzeit ausgeführten Container
    \end{itemize}
\end{itemize}

Die Details zur Implementierung und dem Ausgabeformat während der Ausführung der Simulation sind in \autoref{subsubsec:simulationStepOutput} erläutert, die zur Ausgabe der generellen Testfalldaten in \autoref{sec:implTestcases}. Ein Beispiel einer möglichen Ausgabe für einen Testfall findet sich in \autoref{app:outputFormat}.
