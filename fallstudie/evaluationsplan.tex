\section{Planung der Tests und der Evaluation}
\label{sec:evaluationPlan}

Um das Testsystem zu validieren, wurde zunächst ein Evaluationsplan aufgestellt.
In diesem ist festgehalten, was getestet wird, wie die \glspl{Testkonfiguration} definiert sind, und wie die bei der Ausführung gewonnen Daten organisiert werden.

\subsection{Behauptungen und Variablen}
\label{subsec:theses}

Als Basis für die Behauptungen und Variablen dienten die in \cref{sec:requirements} definierten Anforderungen an das Cluster und das Testsystem.
Sie gehen damit auch einher mit den Behauptungen, welche für die Evaluation aufgestellt werden.
Basierend auf den Anforderungen wurden daher folgende unabhängigen Variablen zur Evaluation ermittelt:

\begin{itemize}
    \item Anzahl der Hosts und Nodes
    \item Anzahl der Clients
    \item Anzahl der \glspl{Testfall} pro Test
    \item \emph{Seed} für Zufallsgeneratoren
    \item Generelle Wahrscheinlichkeit zur Aktivierung und Deaktivierung der Komponentenfehler
\end{itemize}

Basierend auf den unabhängigen Variablen wurden \uA folgende abhängigen Variablen ermittelt:

\begin{itemize}
    \item Anzahl der tatsächlich ausgeführten Testfälle
    \item Aktivierte und deaktivierte Komponentenfehler
    \item Anzahl ausgeführter Anwendungen
    \item Anzahl und Gründe für evtl. nicht vollständig ausgeführte Anwendungen
\end{itemize}

Während die unabhängigen Variablen dazu genutzt werden, um \glspl{Testkonfiguration} zu definieren (vgl. \cref{subsec:testcaseGeneration}), dienen die abhängigen Variablen als wichtige Kennzahlen im Rahmen der Evaluation in \cref{ch:evaluationResults}.

\subsection{Generierung der Testkonfigurationen}
\label{subsec:testcaseGeneration}

Um nun anhand der in \cref{subsec:theses} definierten Variablen die in \cref{sec:requirements} definierten Anforderungen zu prüfen, sind mehrere \glspl{Test} nötig.
Als Basis zur Definition einer \gls{Testkonfiguration} dienen die in \cref{subsec:theses} definierten unabhängigen Variablen, die durch weitere Angaben ergänzt werden:

\begin{itemize}
    \item Anzahl genutzter Hosts
    \item Basisanzahl der Nodes
    \item Anzahl simulierter Clients
    \item Basisseed für Zufallsgeneratoren
    \item Anzahl auszuführender Testfälle
    \item Mindestdauer für einen Testfall
    \item Nutzung einer oder mehreren Mutationen
    \item Generelle Wahrscheinlichkeit zur Aktivierung von Komponentenfehlern
    \item Generelle Wahrscheinlichkeit zur Deaktivierung von Komponentenfehlern
    \item Verwendung von vorab generierten Eingabedaten
\end{itemize}

Die Auswahl der ausgeführten \glspl{Anwendung} erfolgt während der Testausführung anhand des Basisseeds der Testkonfiguration.
Zwar werden durch das Transitionssystem basierend auf den dort definierten Wahrscheinlichkeiten zufällig \glspl{Anwendung} ausgewählt, jedoch kann dies durch den Charakter des im .NET"=Framework vorhandenen Zufallsgenerators gesteuert werden.
Der in .NET implementierte Zufallsgenerator ist nämlich kein \emph{echter}, sondern ein Pseudo"=Zufallsgenerator, der mithilfe eines mathematischen Algorithmus Zufallszahlen berechnet \cite{RandomClassDoc}.
Zwar wird standardmäßig ein zeitbasierter Seed als Startwert für den Algorithmus genutzt \cite{RandomClassDoc}, durch die Angabe eines spezifischen Seeds kann jedoch die Wiederausführbarkeit der \glspl{Test} sichergestellt werden.

Auch die Aktivierung und Deaktivierung von Komponentenfehlern selbst soll während der Ausführung eines Testfalls festgelegt werden, wodurch die Wahrscheinlichkeit für deren Aktivierung bzw. Deaktivierung einen maßgeblichen Einfluss besitzt.
Da Anwendungen, die Eingabedaten für andere \glspl{Anwendung} generieren (vgl. \cref{secsec:clusterSetup}), \uU fehlschlagen können, kann eine \gls{Testkonfiguration} mit vorab generierten und im \gls{HDFS} gespeicherten Daten durchgeführt werden.
Dadurch wird es ermöglicht, dass in solchen \glspl{Test} spätere \glspl{Anwendung} nicht von zuvor ausgeführten \glspl{Anwendung} zur Generierung der Eingabedaten abhängig sind.
Da einige \glspl{Test} zudem als Mutationstests durchgeführt werden sollen, muss für eine \gls{Testkonfiguration} dies entsprechend definiert werden.

Die Auswahl der konkreten \glspl{Testkonfiguration} in \cref{sec:selectTestcases} erfolgte im Rahmen der in \cref{sec:implTestcases} beschriebenen Implementierung der Tests.

\subsection{Organisation und Ausgabe der Daten}
\label{subsec:dataOrganisation}

Damit die bei der Ausführung gewonnenen Daten auch zur Evaluation genutzt werden können, wurde hierzu festgelegt, welche Daten während der Ausführung gespeichert werden.
Alle relevante Daten werden hierzu während der Ausführung der \glspl{Testfall} in einer Log"=Datei gespeichert.
Zur Unterscheidung von einzelnen Ausführungen werden die Daten klar strukturiert.

Beim Start eines Testfalls sollen daher zunächst einige generelle Daten ausgegeben werden:

\begin{itemize}
    \item Basis"=Seed für die Zufallsgeneratoren
    \item Wahrscheinlichkeiten für Aktivierung und Deaktivierung der Komponentenfehler
    \item Anzahl genutzter Hosts, Nodes und Clients
    \item Anzahl der ausgeführten Simulations"=Schritte
    \item Angabe, ob ein normaler \gls{Test} oder ein Mutationstest ausgeführt wird
\end{itemize}

Im Rahmen der Simulation können weitere Daten ausgegeben werden, wie \zB:

\begin{itemize}
    \item Angabe, ob vorab generierte Eingabedaten genutzt werden oder diese während der Ausführung eines Tests generiert werden
    \item Mindestdauer für einen Testfall
    \item Auszuführende Benchmarks pro Client
\end{itemize}

Die Ausgabe der Daten der \gls{YARN}"=Komponenten wird bei jedem erfolgreichen \gls{Testfall} durchgeführt, damit das Verhalten des Clusters berücksichtigt werden kann.
Bei nicht erfolgreichen Testfällen wird die Simulation dagegen beendet.
Für solche Fälle werden nach Abschluss der Simulation erneut die Daten des Clusters ausgelesen und ausgegeben.
Es können hierbei alle Daten ausgegeben werden, welche erkannt werden können, mindestens jedoch:

\begin{itemize}
    \item Für jeden Node:
    \begin{itemize}
        \item ID bzw. Name des Nodes
        \item Aktueller Status
        \item Informationen zur Fehleraktivierung
        \item Anzahl ausgeführter \gls{Container} auf dem Node
        \item Angaben zur Speicherauslastung
        \item Angaben zur CPU"=Auslastung
    \end{itemize}
    
    \item Für jeden Client:
    \begin{itemize}
        \item ID bzw. Name des Clients
        \item Aktuell ausgeführter Benchmark
        \item ID der aktuell ausgeführten \gls{Anwendung} auf dem Cluster
    \end{itemize}

    \item Für jede Anwendung:
    \begin{itemize}
        \item ID der Anwendung
        \item Bezeichnung der Anwendung
        \item Aktueller und finaler Status der Anwendung
        \item ID bzw. Name des Nodes, auf dem der \gls{AppMstr} ausgeführt wird
    \end{itemize}

    \item Für jeden Attempt:
    \begin{itemize}
        \item ID des Attempts
        \item Aktueller Status des Attempts
        \item ID des \gls{AppMstr}"=Containers
        \item ID bzw. Name des Nodes, auf dem der \gls{AppMstr} ausgeführt wird
        \item Anzahl der derzeit ausgeführten Container
    \end{itemize}
\end{itemize}

Die Details zur Implementierung der Ausgaben sind in \cref{subsec:simulationStep} erläutert, die zur Ausgabe der generellen Testdaten in \cref{sec:implTestcases}. Ein Beispiel für den Inhalt einer Logdatei eines Tests findet sich in \cref{app:outputFormat}.
