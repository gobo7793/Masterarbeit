\section{Planung der Tests und der Evaluation}
\label{sec:evaluationPlan}

Um TestingHadoop zu validieren, wurde zunächst ein Evaluationsplan aufgestellt.
In diesem ist festgehalten, was getestet wird, wie die Testkonfigurationen definiert sind und wie die bei der Ausführung gewonnenen Daten organisiert werden.

\subsection{Behauptungen und Variablen}
\label{subsec:theses}

Als Basis für die Behauptungen und Variablen dienten die in \cref{sec:requirements} definierten Anforderungen an das Cluster und das Testsystem.
Sie gehen damit auch einher mit den Behauptungen, welche für die Evaluation aufgestellt werden.
Basierend auf den Anforderungen, wurden daher folgende unabhängige Variablen zur Evaluation ermittelt:

\begin{itemize}
    \item Anzahl der Hosts und Nodes
    \item Anzahl der Clients
    \item Anzahl der Testfälle pro Test
    \item \emph{Seed} für Zufallsgeneratoren
    \item Generelle Wahrscheinlichkeit zur Aktivierung und Deaktivierung der Komponentenfehler
\end{itemize}

Basierend auf den unabhängigen Variablen, wurden \uA folgende abhängige Variablen ermittelt:

\begin{itemize}
    \item Anzahl der tatsächlich ausgeführten Testfälle
    \item Aktivierte und deaktivierte Komponentenfehler
    \item Anzahl ausgeführter Anwendungen
    \item Anzahl und Gründe für evtl. nicht vollständig ausgeführte Anwendungen
\end{itemize}

Während die unabhängigen Variablen dazu genutzt werden, um Testkonfigurationen zu definieren (vgl. \cref{subsec:testcaseGeneration}), dienen die abhängigen Variablen als wichtige Kennzahlen im Rahmen der Evaluation in \cref{ch:evaluationResults}.

\subsection{Generierung der Testkonfigurationen}
\label{subsec:testcaseGeneration}

Um nun anhand der in \cref{subsec:theses} definierten Variablen die in \cref{sec:requirements} definierten Anforderungen zu prüfen, sind mehrere Tests nötig.
Als Basis zur Definition einer Testkonfiguration dienen die in \cref{subsec:theses} definierten unabhängigen Variablen, die durch weitere Angaben ergänzt werden:

\begin{itemize}
    \item Anzahl genutzter Hosts
    \item Basisanzahl der Nodes
    \item Anzahl simulierter Clients
    \item Basisseed für Zufallsgeneratoren
    \item Anzahl auszuführender Testfälle
    \item Mindestdauer für einen Testfall
    \item Nutzung einer oder mehrerer Mutationen
    \item Generelle Wahrscheinlichkeit zur Aktivierung von Komponentenfehlern
    \item Generelle Wahrscheinlichkeit zur Deaktivierung von Komponentenfehlern
    \item Verwendung von vorab generierten Eingabedaten
\end{itemize}

Die Auswahl der ausgeführten Anwendungen erfolgt während der Testausführung anhand des Basisseeds der Testkonfiguration.
Zwar werden durch das Transitionssystem, basierend auf den dort definierten Wahrscheinlichkeiten, zufällig Anwendungen ausgewählt, jedoch kann dies durch den Charakter des im .NET"=Framework enthaltenen Zufallsgenerators gesteuert werden.
Der in .NET implementierte Zufallsgenerator ist nämlich kein \emph{echter}, sondern ein Pseudo"=Zufallsgenerator, der mithilfe eines mathematischen Algorithmus Zufallszahlen berechnet \cite{RandomClassDoc}.
Obwohl standardmäßig ein zeitbasierter Seed als Startwert für den Algorithmus genutzt wird \cite{RandomClassDoc}, kann durch die Angabe eines spezifischen Seeds die Wiederausführbarkeit der Tests sichergestellt werden.

Auch die Aktivierung und Deaktivierung von Komponentenfehlern selbst soll während der Ausführung eines Testfalls festgelegt werden, wodurch die Wahrscheinlichkeit für deren Aktivierung bzw. Deaktivierung einen maßgeblichen Einfluss besitzt.
Da Anwendungen, die Eingabedaten für andere Anwendungen generieren, \uU fehlschlagen können, kann eine Testkonfiguration mit vorab generierten und im HDFS gespeicherten Daten durchgeführt werden.
Dadurch wird es ermöglicht, dass in solchen Tests spätere Anwendungen nicht von zuvor ausgeführten Anwendungen zur Generierung der Eingabedaten abhängig sind.
Ebenfalls definiert werden müssen die Tests, die als Mutationstests durchgeführt werden sollen.
Hierbei müssen auch die Mutationen selbst definiert werden, die vom Testsystem erkannt werden sollen.

Die Auswahl der konkreten Testkonfigurationen in \cref{sec:selectTestcases} erfolgte im Rahmen der in \cref{sec:implTestcases} beschriebenen Implementierung der Tests.
Die implementierten und genutzten Mutationen werden bereits in \cref{sec:implMutationTests} erläutert.

\subsection{Organisation und Ausgabe der Daten}
\label{subsec:dataOrganisation}

Damit die bei der Ausführung gewonnenen Daten auch zur Evaluation genutzt werden können, wurde hierzu festgelegt, welche Daten während der Ausführung gespeichert werden.
Alle relevanten Daten werden hierzu während der Ausführung der Testfälle in einer Log"=Datei gespeichert.
Zur Unterscheidung von einzelnen Ausführungen werden die Daten klar strukturiert.
Beim Start eines Tests sollen daher zunächst einige generelle Daten ausgegeben werden:

\begin{itemize}
    \item Basis"=Seed für die Zufallsgeneratoren
    \item Wahrscheinlichkeiten für Aktivierung und Deaktivierung der Komponentenfehler
    \item Anzahl genutzter Hosts, Nodes und Clients
    \item Anzahl der auszuführenden Testfälle
    \item Angabe, ob ein normaler Test oder ein Mutationstest ausgeführt wird
\end{itemize}

Im Rahmen der Simulation können weitere Daten ausgegeben werden, wie \zB:

\begin{itemize}
    \item Angabe, ob vorab generierte Eingabedaten genutzt oder diese während der Ausführung eines Tests generiert werden
    \item Mindestdauer für einen Testfall
    \item Auszuführende Benchmarks pro Client
\end{itemize}

Die Ausgabe der Daten der YARN"=Komponenten wird bei jedem erfolgreichen Testfall durchgeführt, damit das Verhalten des Clusters berücksichtigt werden kann.
Bei nicht erfolgreichen Testfällen wird die Simulation dagegen beendet.
Für solche Fälle werden nach Abschluss der Simulation erneut die Daten des Clusters ausgelesen und ausgegeben.
Es können hierbei alle Daten ausgegeben werden, welche erkannt werden können, mindestens jedoch:

\begin{itemize}
    \item Für jeden Node:
    \begin{itemize}
        \item ID bzw. Name des Nodes
        \item Aktueller Status
        \item Informationen zur Fehleraktivierung
        \item Anzahl ausgeführter Container auf dem Node
        \item Angaben zur Speicherauslastung
        \item Angaben zur CPU"=Auslastung
    \end{itemize}
    
    \item Für jeden Client:
    \begin{itemize}
        \item ID bzw. Name des Clients
        \item Aktuell ausgeführter Benchmark
        \item ID der aktuell ausgeführten Anwendung auf dem Cluster
    \end{itemize}

    \item Für jede Anwendung:
    \begin{itemize}
        \item ID der Anwendung
        \item Bezeichnung der Anwendung
        \item Aktueller und finaler Status der Anwendung
        \item ID bzw. Name des Nodes, auf dem der \gls{AppMstr} ausgeführt wird
    \end{itemize}

    \item Für jeden Attempt:
    \begin{itemize}
        \item ID des Attempts
        \item Aktueller Status des Attempts
        \item ID des \gls{AppMstr}"=Containers
        \item ID bzw. Name des Nodes, auf dem der \gls{AppMstr} ausgeführt wird
        \item Anzahl der derzeit ausgeführten Container
    \end{itemize}
\end{itemize}

Die Details zur Implementierung der Ausgaben sind in \cref{subsec:simulationStep} erläutert, die zur Ausgabe der generellen Testdaten in \cref{sec:implTestcases}. Ein Beispiel für den Inhalt einer Logdatei eines Tests findet sich in \cref{app:outputFormat}.
