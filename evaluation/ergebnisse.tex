\section{Erkenntnisse der Evaluation}
\label{sec:evaluationResults}

\todo{Zusammenfassung, hier erwähnen, dass MUT schneller und weniger failapps hatte?}

\subsection{Betrachtung der \ac{MARP}"=Werte}
\label{sec:marpValueResults}

Bei der Betrachtung der \ac{MARP}"=Werte lässt sich generell sagen, dass die Selfbalancing"=Komponente den \ac{MARP}"=Wert entsprechend der Auslastung des Clusters anpasst.
Während bei allen Testkonfigurationen, bei denen alle 4 Mutationen aktiv waren, der \ac{MARP}"=Wert unverändert blieb, wurde er bei 16 von 20 Ausführungen der 16 Konfigurationen ohne Mutationen verändert:

\begin{table}[h]
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
    	Konf. &  1.1  &  1.2  &   3   &  5.1  &  5.2  &  7.1  &  7.2  &  9.1  &  9.2  &  11   \\ \hline
    	Wert  & 0,100 & 0,100 & 0,474 & 0,242 & 0,100 & 0,100 & 1,000 & 0,269 & 0,175 & 0,539 \\
    	\multicolumn{11}{c}{} \\
    	Konf. &  13   &  15   &  17   &  19   &  21   &  23   &  25   &  27   &  29   &  31   \\ \hline
    	Wert  & 0,356 & 0,368 & 0,731 & 0,430 & 0,335 & 0,498 & 0,521 & ,0819 & 0,273 & 0,333
    \end{tabular}
    \caption{Finale \ac{MARP}"=Werte der Testkonfigurationen ohne Mutanten}
    \label{tab:finalMarpValues}
\end{table}

Da er in den Konfigurationen 1 und 7 bei der jeweils ersten Ausführung nicht verändert wurde, wurden beide Konfigurationen erneut ausgeführt, wobei der \ac{MARP}"=Wert bei letzterem mehrmals erhöht wurde, bevor er im finalen Clusterstatus auf 1 gesetzt wurde.
Bei der Konfiguration 1 wurde der Wert dagegen bei keiner der beiden Ausführungen verändert.

Die nicht durchgeführte Änderung des \ac{MARP}"=Wertes in Konfiguration 1 liegt sehr wahrscheinlich daran, dass in hier nur im ersten der fünf Testfälle zwei Anwendungen gleichzeitig gestartet werden.
Dadurch wurden in allen 10 Testfällen zusammen nur 8 Anwendungen gestartet, die Hälfte davon jeweils beim ersten Testfall.
Da zudem 4 der 8 Anwendungen nur kleine Anwendungen (\acl{rtw} und \acl{pi}) sind und diese entsprechend schnell beendet werden können, steht den wesentlich ressourcenintensiveren Anwendungen \acl{dfw} und \acl{dfr} das gesamte Cluster nahezu exklusiv zur Verfügung.
Daher stehen den Anwendungen ausreichend Ressourcen zur Verfügung, was eine Anpassung des \ac{MARP}"=Wertes unnötig erscheinen lässt und daher durch die Selfbalancing"=Komponente nicht durchgeführt wird.

Bei der Testkonfiguration 7 ist dies ähnlich, wobei die gesamte Last auf mehr Nodes verteilt werden kann.
Der Test 7.2 und der hier deutlich veränderte \ac{MARP}"=Wert im Vergleich zu Test 7.1 ohne Anpassung zeigt jedoch auch, dass es stark abhängig davon ist, wie die Last im Cluster verteilt wird.
Bestätigt wird dies durch die Tests 9.1 und 9.2, da bei letzterem weniger Komponentenfehler injiziert wurden und sich die Last entsprechend besser verteilen konnte.
Dadurch war im Test 9.2 ein um rund 0,1 niedrigerer \ac{MARP}"=Wert als im Test 9.1 nötig.

Auffällig war zudem, dass der \ac{MARP}"=Wert in den Testausführungen 7.2, 9.1 und 23 nicht direkt im ersten Testfall verändert wurde, sondern erst bei der Ausführung von Testfällen im späteren Verlauf der jeweiligen Tests.
Als Resultat wurde daher in 9 der 15 Testfälle zunächst das hierfür genutzte Constraint als ungültig validiert.

\subsection{Erkennung der Mutanten}
\label{sec:killingMutants}

% Generelles, Mutanten wurden fast immer erkannt
Da die Selfbalancing"=Komponente den \ac{MARP}"=Wert basierend auf der aktuellen Auslastung des Clusters anpasst, konnte anhand der Betrachtung der \ac{MARP}"=Werte auch geprüft werden, ob die implementierten Mutationen vom Testsystem erkannt wurden.
Um das zu bewerkstelligen wurden von den 16 Testkonfigurationen mit einem Mutationsszenario 22 Testausführungen durchgeführt (vgl. \autoref{app:overviewExecutedTestCases}).

Zunächst wurde das Mutationsszenario genutzt, in dem alle vier Mutationen enthalten sind (vgl. \autoref{sec:implMutationTests}).
Bei jeder der 17 Testausführungen mit allen Mutanten wurden diese basierend auf dem Constraint zur Erkennung des \ac{MARP}"=Wertes erkannt.
Eine Besonderheit bildet hier jedoch der Test 2, der den korrespondierenden Mutationstest zur Konfiguration 1 darstellt, bei der bei beiden Ausführungen der \ac{MARP}"=Wert nicht verändert wurde.
Bei Test 2 kann daher nicht eindeutig festgestellt werden, ob der Mutant erkannt wurde, oder ob aufgrund der gestarteten Anwendungen der \ac{MARP}"=Wert nicht verändert wurde (vgl. Vermutungen zu Testkonfiguration 1 in \autoref{tab:finalMarpValues}).

Anders ist dies im Vergleich der Ausführungen der Testkonfigurationen 7 und 8.
Durch die massive Veränderung des \ac{MARP}"=Wertes im Test 7.2 auf den finalen Wert von 1 kann davon ausgegangen werden, dass die Mutanten der Konfiguration 8 erkannt wurden.
Dies wird dadurch gestützt, dass bei Konfiguration 8 im Gegensatz zur korrespondieren Testkonfiguration 3 der 4 gestarteten Anwendungen gefailt sind.
Zudem stellt das ein Indiz dafür dar, dass der Mutant in Konfiguration 2 erkannt worden sein könnte.

% Warum wurden mutanten in #10 wohl erkannt?
Während bei jeder Konfiguration ein Mutationsszenario mit jeweils allen vier Mutanten genutzt wurde, wurde die Testkonfiguration 10 zusätzlich mit jeweils einem Mutanten ausgeführt.
Ziel hierbei war es zu validieren, ob einzelne Mutanten ebenfalls vom Testsystem erkannt werden oder zur Erkennung der Mutanten vom Testsystem eine Kombination aus mehreren Mutaten nötig ist.
Hierzu wurde die Testkonfiguration 10 mit unterschiedlichen Mutationsszenarien der Plattform Hadoop"=Benchmark ausgeführt, bei denen jeweils ein Mutant aktiv ist.
Die Auswahl dieser Testkonfiguration hierfür liegt darin begründet, dass hier das Cluster auf beiden Hosts mit sechs Nodes gestartet wird, auf denen bis zu vier Anwendungen gleichzeitig gestartet werden.
Zudem wurde beim Test 9.2 festgestellt, dass sich der \ac{MARP}"=Wert auch direkt im ersten ausgeführten Testfall ändern kann und es gab bei den Ausführungen der Konfiguration 9 keine weiteren ungültigen Constraints als fehlerhafte Anwendungen.

Einige Ergebnisse der hierfür 5 ausgeführten Tests sind sehr unterschiedlich.
So variiert die Anzahl der aktivierten und deaktivierten Komponentenfehler zwischen 7 und 11 bzw. 5 und 9, sowie die Anzahl der gefailten Anwendungen zwischen 1 und 3.
Gemein haben alle Tests jedoch, dass der \ac{MARP}"=Wert bei allen 5 Tests nicht verändert wurde, womit alle Mutationen erkannt wurden.
Damit kann festgestellt werden, dass jeder der vier in \autoref{sec:implMutationTests} beschriebenen Mutanten durch das entwickelte Testsystem erkannt wird.

\subsection{Aktivierung und Deaktivierung der Komponentenfehler}
\label{sec:faultInjectionEval}

Die Aktivierung und Deaktivierung der Komponentenfehler in einem Testfall hängt neben dem zur Berechnung benötigtem Seed vor allem von den zuvor ausgeführten Testfällen einer Testkonfiguration ab (vgl. \autoref{sec:simulationFaultActivation}).
Daher werden abhängig von der Lastverteilung im Cluster auch bei einer mehrmaligen Ausführung der gleichen Konfiguration \uU unterschiedliche Komponentenfehler aktiviert.
Unterschieden werden muss hierbei zudem zwischen aktivierten und injizierten Komponentenfehlern.
Während beide implementierten Komponentenfehler für einen Node in einem Testfall auch gleichzeitig aktiviert werden konnten, wurde gemäß \todo{Abschnitt mit Komponentenfehler im Node} in so einem Fall jedoch nur der \texttt{NodeDead}"=Fehler im Cluster injiziert.
Die Deaktivierung bzw. das Reparieren der Komponentenfehler verhält sich analog hierzu.

Im Vergleich zwischen korrespondierenden Konfigurationen, die sich nur in der Nutzung des Mutationsszenarios unterschieden, wurde nur 5 mal die gleiche Anzahl an Komponentenfehler aktiviert, bei der Deaktivierung der Komponentenfehler besitzen nur 3 korrespondierende Konfigurationen die gleiche Anzahl.
Die Anzahl der aktivierten und deaktivierten Komponentenfehler unterschied sich dagegen in 8 bzw. 7 korrespondierenden Testkonfigurationen um jeweils einen Komponentenfehler.
In allen anderen Ausführungen von korrespondierenden Konfigurationen unterschied sich die Anzahl um jeweils mehr als einen Komponentenfehler.
Mit 20 aktivierten Komponentenfehlern wurden bei der Ausführung der Testkonfiguration 32 die meisten aktiviert, die meisten Komponentenfehler deaktiviert wurden bei den Konfigurationen 11 und 12 mit jeweils 15 Stück.
Nur im Test zur Konfiguration 2 wurden mit 3 Stück alle aktivierten Komponentenfehler während der Simulation auch wieder deaktiviert.
In den Tests 4, 5.1, 5.2 und 6 wurden jeweils 6 oder 7 Komponentenfehler aktiviert, jedoch keine deaktiviert, weshalb diese Tests bereits beim 3. ausgeführten Testfall gemäß \autoref{sec:simulationOracle} abgebrochen wurden.

Im Vergleich zwischen den Tests von korrespondierenden Testkonfigurationen sind die Tests der Konfigurationen 1 und 2 auffällig.
Während beim Test 1.1 mit 5 Komponentenfehlern bzw. beim Test 1.2 mit 7 Komponentenfehlern jeweils rund jeder achte mögliche Komponentenfehler aktiviert wurde, wurden beim Test 2 lediglich 3 Komponentenfehler für 4 Nodes in 5 Testfällen (insgesamt also 40 mögliche Komponentenfehler) aktiviert.
Eine geringere Quote weist lediglich Test 9.2 auf, bei dem mit 4 von 60 möglichen Komponentenfehler nur 7 \% aktiviert wurden.
Die Testkonfiguration 9 ist auch deshalb auffällig, da im Test 9.1 fast dreimal so viele Komponentenfehler, also 11 Stück, aktiviert wurden.
Auch in den korrespondierenden Tests der Konfiguration 10 liegt die Anzahl der aktivierten Komponentenfehler mit 7 bis 11 jeweils mehr als doppelt so hoch wie in Test 9.2.

Auffällig ist zudem, dass bei korrespondierenden Testkonfigurationen mit unterschiedlicher Anzahl an aktivierten Komponentenfehlern die niedrigere Anzahl meist die Mutationstests aufweisen.
Nur bei den Tests 27 und 28.1 sowie bei den Tests 31 und 32 weisen die Tests ohne Mutationsszenario die niedrigere Anzahl auf.
\todo{auf diskussion der selfbalancing komponenten verweisen als grund}
\todo{Unterschied aktiviert/injiziert}

\subsection{Nicht vollständig abgeschlossene Anwendungen}
\label{sec:failedAppsEval}

Wie bereits erwähnt, sind rund ein viertel aller gestarteten Anwendungen gefailt, was im Schnitt 2,4 Anwendungen pro ausgeführten Test ergibt.
Die meisten gefailten Anwendungen sind hierbei mit 9 bzw. 8 bei den Tests 31 und 32 zu finden.
Auffällig ist zudem der Vergleich zwischen den Tests 19 und 20.
Während bei der Ausführung der Testkonfiguration 19 ganze 5 Anwendungen gefailt sind, ist bei der Ausführung des Tests 20 keine einzige gefailt.
Auffallend ist zudem, dass bei Konfigurationen mit Mutationsszenario fast immer weniger oder gleich viele Anwendungen gefailt sind als bei den korrespondierenden Konfigurationen ohne Mutationsszenario.
Eine Ausnahme bildet der Test 8, bei dem 3 Anwendungen gefailt sind, während bei den Tests 7.1 und 7.2 jeweils keine Anwendung gefailt ist.
Eine weitere Ausnahme bildet der Test 9.2, bei dem eine Anwendung mehr gefailt ist als im Test 10.3, die restlichen Tests der Konfigurationen 9 und 10 verhalten sich jedoch wie andere korrespondierende Testkonfigurationen.

Bei der Betrachtung der Constraints, welche die in \autoref{sec:clusterRequirements} definierte Anforderung umsetzen, dass Anwendungen vollständig ausgeführt werden, solange sie nicht manuell bzw. durch das Testsystem vorzeitig abgebrochen werden, fällt auf, dass die Anzahl der ungültigen Validierungen durch das Oracle mit kumuliert 317 ungültigen Constraints mehr als die Hälfte aller ungültigen Constraints ausmacht (58,6 \% ).
Im Schnitt ergibt das somit 7,5 ungültige Constraints pro Test bzw. 1,4 ungültige Constraints pro durch das Oracle überprüften Testfall.
Die auf den ersten Blick sehr hohe Anzahl an ungültigen Constraints resultiert daraus, dass eine gefailte Anwendung bei jedem nachfolgenden Testfall bei einer Testausführung erneut durch das Oracle entsprechend validiert wurde.
Aussagekräftiger ist daher die Anzahl von 102 nicht vollständig abgeschlossenen Anwendungen.

Anhand der Datenbasis lassen sich vier Ursachen für nicht vollständig ausgeführte Anwendungen ausmachen:

\begin{itemize}
    \item Der \ac{AppMstr} ist nicht mehr erreichbar, da der auszuführende Node aufgrund eines Komponentenfehlers nicht mehr erreichbar ist und der \ac{AppMstr} nach einiger Zeit mit dem Fehler \emph{\ac{AppMstr}"=Timeout} als abgebrochen markiert wird
    \item Die den \ac{AppMstr} zugewiesenen Nodes sind vollständig ausgelastet, wodurch der \ac{AppMstr} selbst nicht ausgeführt werden kann und dieser nach einiger Zeit mit dem Fehler \emph{\ac{AppMstr}"=Timeout} abgebrochen wird
    \item Während der Ausführung einer \ac{MR}"=Anwendung wird ein Fehler im Map"=Task festgestellt, der dazu führt, dass der Task abgebrochen wird.
    Dieser Fehler kam bei den hier ausgeführten Tests lediglich bei der Anwendung \acl{dfr} vor, wenn die zuvor generierten Eingabedaten für diesen Benchmark aufgrund aktivierter Komponentenfehler nicht mehr im Cluster vorhanden waren.
    Zwar werden Dateien im \ac{HDFS} immer auf mehr als einem Node gespeichert (vgl. \autoref{sec:hadoop}), jedoch ist es möglich, dass die für die Anwendung benötigten Daten auf Nodes repliziert wurden, die alle beendet wurden.
    Dies führte dazu, dass die benötigten Daten nicht gefunden werden können bzw. bereits im \ac{HDFS} als fehlerhaft markiert sind.
    Dadurch wird im Map"=Task ein Fehler ausgelöst, der die gesamte Anwendung vorzeitig beendet.
    \item Der \ac{AppMstr} wird mit dem Exitcode -100 beendet\todo{why???}
\end{itemize}

Hierbei werden aufgrund eines \ac{AppMstr}"=Timeouts zunächst nur die Attempts mit dem entsprechenden Fehler abgebrochen, nicht jedoch die Anwendung selbst.
Die Anwendung selbst wird in so einem Fall erst dann als gefailt abgebrochen, sobald zwei Attempts aufgrund eines Timeouts abgebrochen werden mussten.
Wenn ein Attempt mit dem Exitcode -100 terminiert, wird unabhängig von zuvor ausgeführten Attempts ein erneuter Attempt mit entsprechendem \ac{AppMstr} gestartet.

% Kuriose Steps: AppMstr auf zb Node1 -> Stop -> AppMstr zb Node 5 -> Stop -> App Failed
Bei einigen der \ac{AppMstr}"=Timeouts aufgrund der Aktivierung von Komponentenfehler lässt sich zudem ein spezielles Muster erkennen.
Hierbei wurde in einem zuvor ausgeführten Testfall regulär auf einem Node ein \ac{AppMstr} einer Anwendung allokiert.
Nun kann es passieren, dass für diesen Node ein Komponentenfehler injiziert wird, was dazu führt, dass der Node nicht mehr erreichbar ist und der \ac{AppMstr} aufgrund eines Timeouts als beendet markiert wird.
Hierbei wird direkt im Anschluss ein neuer \ac{AppMstr} allokiert, was auch dazu führt, dass die Anwendung nun einen zweiten Attempt besitzt, nachdem der erste aufgrund des Timeouts abgebrochen wurde.
Nun kann es dabei passieren, dass dies noch während der Aktivierung von Komponentenfehlern innerhalb eines Simulations"=Schrittes geschieht, wodurch es möglich ist, dass der auszuführende Node des zweiten \ac{AppMstr} ebenfalls aufgrund eines im gleichen Testfall injizierten Komponentenfehlers nicht mehr erreichbar ist.
Dadurch wird der zweite \ac{AppMstr} bzw. Attempt aufgrund des Timeouts vorzeitig als abgebrochen markiert und die gesamte Anwendung dadurch gefailt.

\subsection{Rekonfiguration nicht möglich}
\label{sec:noReconfig}

% Rekonffehler #3-6

% Rekonffehler #15/16

% Rekonffehler #19-22

% Rekonffehler #27/28

% Rekonffehler #31/32

\subsection{Nicht erkannte oder gespeicherte \ac{AppMstr}"=Nodes}
\label{sec:noDetectedHost2}

% NotSaved #7/8

% NoAllok #8

% NoSaved #23-26

\subsection{Nicht erkannte, injizierte bzw. reparierte Komponentenfehler}
\label{sec:noDetectedFault}

% SuT/Test-Constraint StartNode 4 bei #17-28

\subsection{Nicht gestartete Anwendungen}
\label{sec:notStartedApps}

% Multiapp-Constraint bei #29-32

% Zu wenig Submitter bei #31/32
