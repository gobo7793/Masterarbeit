\section{Entwicklung des Treibers}
\label{sec:sshDriver}
\todo{Multihost-Mode irgendwo erklären}

In \cref{sec:modelArchitecture} wurde bereits aufgezeigt, dass der Treiber zur Verbindung des \ac{YARN}"=Modells mit dem realen Cluster aus den drei Komponenten Parser, Connector und der SSH"=Verbindung selbst besteht.
Der Treiber ist im \ac{YARN}"=Modell mithilfe verschiedener Interfaces zur Nutzung des Parsers und Connectors eingebunden.
Da \ac{YARN} mithilfe von Befehlen für die \ac{CLI} und einer REST"=API zwei unterschiedliche Schnittstellen zum Auslesen der Daten der \ac{YARN}"=Komponenten für das Monitoring bereitstellt, wurden jeweils zwei entsprechende Parser und Connectoren hierfür entwickelt.
Andere Befehle wie \zB \ac{HDFS}"=Befehle können ebenfalls mithilfe des entwickelten \ac{CLI}"=Connectors ausgeführt werden, da Connectoren mithilfe von SSH"=Verbindungen mit den Cluster"=Hosts verbunden sind.

\subsection{Grundlegender Aufbau und Integration im \acs{YARN}"=Modell}
\label{subsec:driverModelIntegration}

Zur Integration des Treibers im \ac{YARN}"=Modell stellt dieser mehrere Interfaces bereit.
Dadurch sind einerseits der Treiber und das \ac{YARN}"=Modell strikt getrennt, andererseits wird es dadurch auch ermöglicht, in Zukunft andere Möglichkeiten als die hier Entwickelten zur Interaktion mit dem realen Cluster zu entwickeln und zu nutzen.

Zur Interaktion des \ac{YARN}"=Modells mit dem Treiber werden dem Modell folgende Interfaces zur Verfügung gestellt:

\begin{itemize}
    \item \texttt{IHadoopParser} für Parser
    \item \texttt{IHadoopConnector} für Connectoren
    \item Von \texttt{IParsedComponent} abgeleitete Interfaces für geparste \ac{YARN}"=Komponenten:
    \begin{itemize}
        \item \texttt{IApplicationResult} für Anwendungen
        \item \texttt{IAppAttemptResult} für Attempts
        \item \texttt{IContainerResult} für Anwendungs"=Container
        \item \texttt{INodeResult} für Nodes
    \end{itemize}
\end{itemize}

Das Monitoring der Daten des realen Clusters wird mithilfe des Parsers durchgeführt.
Das Interface \texttt{IHadoopParser} stellt hierfür entsprechende Parsing"=Methoden für die vier implementierten \ac{YARN}"=Komponenten sowie der Übersichtslisten aller einer \ac{YARN}"=Komponente untergeordneten Subkomponenten.
Zudem stellt das Parser"=Interface eine Methode zum Auslesen des aktuellen \ac{MARP}"=Wertes des Schedulers bereit.
Beim Monitoring werden immer die entsprechenden von \texttt{IParsedComponent} abgeleiteten Interfaces zur Rückgabe der ermittelten Daten genutzt.
Hierfür stellen diese Interfaces entsprechende Eigenschaften bereit, um alle mithilfe der \ac{CLI} oder der REST"=API auslesbaren Daten an das \ac{YARN}"=Modell übergeben zu können.

Das Connector"=Interface \texttt{IHadoopConnector} stellt alle zum Abrufen der Daten oder weiteren Interaktion wie das Injizieren von Komponentenfehlern oder Starten von Anwendungen benötigten Methoden und Befehle bereit.
Hierbei wird für das Monitoring unterschieden, ob die Daten vom \ac{TLS} oder vom \ac{RM} von Hadoop abgerufen werden.
Dies ist vor allem bei der Nutzung der REST"=API wichtig, da sich hier die Adressen und Pfade unterscheiden, während bei der Benutzung der \ac{CLI} die Befehle gleich sind .
Der \ac{TLS} wird zum Abrufen der Daten vor allem aus dem Grund genutzt, da hierbei zusätzliche Daten ermittelt werden können, die bei der reinen Nutzung der Schnittstellen des \ac{RM} nicht zurückgegeben werden würden.
Ausgenommen sind hiervon Anwendungen, bei denen die Nutzung des \ac{TLS} keine weiteren Daten von Hadoop zurückgegeben werden\cite{HadoopYarnTlServer271,HadoopYarnCmds271,HadoopRmApi271,HadoopNmApi271}.
Aus diesem Grund ist die Nutzung des \ac{TLS} zum Monitoring von Anwendungen mithilfe des Connector"=Interfaces nicht möglich.

Die Implementierten Parser und Connectoren sind jeweils als Singleton realisiert und sind von der Serialisierung des \ac{YARN}"=Modells durch \ac{ss} ausgenommen.
Dies liegt vor allem darin begründet, dass dadurch Speicher eingespart wird, der dadurch für andere \ac{YARN}"=Komponenten zur Verfügung steht.
Aber auch weitere Einschränkungen durch \ac{ss} spielten eine Rolle (vgl. \cref{sec:ssharp}).
Ein weiterer Vorteil liegt zudem darin, dass für unterschiedliche Einsatzzwecke der gleiche Connector genutzt werden kann, und somit auch die einzelnen vom Connector benötigten SSH"=Verbindungen für unterschiedliche Einsatzzwecke wiederverwendet werden können.
Die Initialisierung der Parser und Connectoren erfolgt jeweils beim ersten Aufruf der Singletons.
Dies geschieht innerhalb des Testsystems üblicherweise entweder beim Initialisieren des Tests (beschrieben in \cref{sec:implTestcases}) oder beim Initialisieren des \ac{YARN}"=Modells selbst durch die Klasse \texttt{Model}.
Die Initialisierung des Modells stellt zudem den einzigen Zeitpunkt dar, bei dem im \ac{YARN}"=Modell direkt mit den implementierten Parsern und Connectoren interagiert wird, jede andere Interaktion findet stattdessen gekapselt mithilfe der Interfaces statt.

Die drei Komponenten des Treibers sind zudem untereinander  voneinander gekapselt.
Bei der Ausführung des Parsers wird daher analog nur zur Initialisierung mit dem konkreten Connector interagiert, während das Connector"=Interface zum Monitoring als einzige Schnittstelle dient.
Da für die SSH"=Verbindung kein eigenes Interface zur Kapselung existiert, ist die Kapselung der Connectoren und der bereitstellenden Klasse der SSH"=Verbindung nicht so streng wie zwischen anderen Komponenten.
Dennoch werden Befehle auf den Cluster"=Hosts ausschließlich mithilfe des Connectors durchgeführt.

\subsection{Entwicklung der Parser}
\label{subsec:implementedParsers}

Der Parser dient dazu, die von Hadoop zurückgegebenen Daten der \ac{YARN}"=Komponenten einzulesen und dem Modell zu übergeben.
Zur Datenhaltung innerhalb der Parser"=Komponente des Treibers wurden hierfür entsprechende Klassen entwickelt, welche die von \texttt{IParsedComponent} abgeleiteten Interfaces implementieren.
Dadurch sind die Datenhaltungs"=Klassen auch direkt dafür geeignet, die Daten an das Modell zu übergeben.

Der grundlegende Ablauf der Initialisierung und dem Abrufen und Konvertieren der Daten ist bei beiden implementierten Parser gleich.
Beim Initialisieren des Parsers wird zunächst der benötigte, passende Connector initialisiert.
Beim Abrufen und Konvertieren der Daten werden zunächst, sofern benötigt, die IDs von weiteren benötigten \ac{YARN}"=Komponenten ermittelt, bevor die Rohdaten durch den Connector ermittelt werden.
Die Rohdaten werden anschließend konvertiert und in der entsprechenden Datenhaltungs"=Klasse gespeichert, welche mithilfe der entsprechenden von \texttt{IParsedComponent} abgeleiteten Interfaces zurückgegeben.

Da Hadoop zwei Schnittstellen in Form der \ac{CLI} und der REST"=API bereit stellt, wurden hierfür entsprechend zwei Parser entwickelt, um die jeweiligen Daten einzulesen.

\subsubsection{Implementierung des CmdParsers}
\label{subsubsec:implCmdParser}

Der erste der beiden entwickelten Parser ist der \texttt{CmdParser} zum Monitoring mithilfe von Befehlen auf der \ac{CLI}.
Zum Auslesen der Daten selbst nutzt der \texttt{CmdParser} den dazugehörigen \texttt{CmdConnector} (vgl. \cref{subsubsec:implCmdConnector}), mit dem zum Auslesen der Daten die entsprechenden \ac{CLI}"=Befehle ausgeführt werden.
Die hierbei zurückgegebenen Daten sind im Vergleich zu den von der REST"=API zurückgegebenen im Umfang deutlich reduziert, in den jeweiligen Übersichten der Subkomponenten zudem auf das notwendigste beschränkt.
Im Gegenzug zur REST"=API werden hier die Daten des \ac{RM} und \ac{TLS} kombiniert ausgegeben.
Eine kurze Übersicht über die Befehle und deren Ausgaben ist im \cref{app:hadoopCmds} zu finden.

Ausgewertet werden die von Hadoop zurückgegebenen Daten mithilfe von \acp{Regex}.
Da das Ausgabeformat jeweils in Listenform oder als ausführlicher Report immer das gleiche Format aufweist, wurden hierfür zwei generische Regex"=Pattern entwickelt, welche zur Auswertung fast aller Daten ausreichend sind:

\begin{lstlisting}[label=lst:cmdRegexPattern,style=cs,
caption={[Implementierte \acs{Regex}"=Pattern im CmdParser]
    Implementierte \acs{Regex}"=Pattern im \texttt{CmdParser}}]
Regex _GenericListRegex = new Regex(@"\s*([^\t]+)");
Regex _GenericDetailsRegex = new Regex(@"\t(.+)\s:\s([^\t]*)[\n\r]", RegexOptions.Multiline);
\end{lstlisting}

Bei zurückgegebenen Listen müssen diese zur Auswertung zunächst zeilenweise getrennt werden, bevor das Regex"=Pattern genutzt werden kann.
Anschließend können durch die Reihenfolge der jeweiligen Regex"=Matchgruppen die jeweiligen Daten der Komponente den entsprechenden Eigenschaften der Datenhaltungsklassen zugeordnet werden.

Eine Besonderheit bilden Zeitstempel.
Diese werden von Hadoop bei der Nutzung der \ac{CLI} meist in Form des Java"=Timestamps in Millisekunden seit dem 1. Januar 1970 00:00:00 Uhr, in der Liste der ausgeführten Container eines Attempts stattdessen im Format \texttt{ddd MMM dd HH:mm:ss zz00 yyyy}.
Dies entspricht \zB dem Zeitstempel \texttt{Fri Jan 05 11:08:16 +0000 2018}.
Weiterführende Informationen zur Formatierung von Zeitstempeln in .NET finden sich in \cite{CsTimeFormatStrings}.
Der von Hadoop zurückgegebene Zeitstempel muss in beiden Fällen zunächst in ein .NET"=kompatibles Format umgewandelt werden.
Dies geschieht mithilfe der Methode \texttt{ParseJavaTimestamp()}, für die mehrere Überladungen implementiert wurden:

\begin{lstlisting}[label=lst:parseJavaTimestamp,style=cs,
caption={[Überladungen der Methode ParseJavaTimestamp()]
    Überladungen der Methode \texttt{ParseJavaTimestamp()}.
    Es steht zudem eine weitere Überladung zur Verfügung, um den Timestamp in Form der Millisekunden seit 1970 als \texttt{string} zu übergeben.
    Dabei wird der \texttt{string} in einen \texttt{long} konvertiert und anschließend die erste hier gezeigte Überladung aufgerufen.}]
public static DateTime ParseJavaTimestamp(long javaMillis)
{
  if(javaMillis < 1)
  return DateTime.MinValue;
  var javaTimeUtc = new DateTime(1970, 1, 1, 0, 0, 0,
     DateTimeKind.Utc).AddMilliseconds(javaMillis);
  return javaTimeUtc.ToLocalTime();
}

public static DateTime ParseJavaTimestamp(string value,
   string format, CultureInfo culture = null)
{
  culture = culture ?? new CultureInfo("en-US");
  DateTime time;
  DateTime.TryParseExact(value, format, culture,
     DateTimeStyles.AssumeUniversal, out time);
  return time;
}
\end{lstlisting}

Die hierbei zurückgegebenen \texttt{DateTime}"=Instanzen werden anschließend zum Speichern der Zeitstempel genutzt.

Die Speicherung und Übergabe der ausführenden Nodes des \ac{AppMstr} oder der Container an das Modell geschieht direkt als entsprechende Node"=Instanz innerhalb des Modells.
Hierfür wird die ID bzw. die URL des Nodes genutzt, um die korrespondierende Instanz im \ac{YARN}"=Modell zu ermitteln und zu speichern.

\subsubsection{Implementierung des RestParsers}
\label{subsubsec:implRestParser}

Der zweite entwickelte Parser ist der \texttt{RestParser}.
Er dient dazu, die mithilfe der REST"=API ermittelten Daten auszuwerten und an das \ac{YARN}"=Modell zu übergeben.
Zum Auslesen der Daten aus dem Cluster wurde hierfür der \texttt{RestConnector} entwickelt (vgl. \cref{subsubsec:implRestConnector}).
Die REST"=API besitzt, auch im Vergleich zur \ac{CLI}"=Schnittstelle, einige Besonderheiten, auf die bei der Implementierung geachtet werden musste \cite{HadoopYarnCmds271,HadoopRmApi271,HadoopNmApi271,HadoopYarnTlServer271}:

\begin{itemize}
    \item Die zurückgegebenen Daten sind deutlich Umfangreicher als bei der \ac{CLI}"=Schnittstelle
    \item Die Daten können im XML- oder JSON"=Format zurückgegebenen werden
    \item Attempts können nur als Liste aller Attempts einer Anwendung zurückgegeben werden
    \item Daten zu Containern können nur durch die \ac{NM} der ausführenden Nodes ermittelt werden
    \item Die Adressen und Pfade von \ac{RM}, \ac{NM} und \ac{TLS} unterscheiden sich
    \item Die Daten von \ac{RM} und \ac{NM} sind immer umfangreicher als die des \ac{TLS}
    \item Der \ac{TLS} enthält für Attempts und Anwendungs"=Container jedoch zusätzliche Daten
    \item Es werden bei Listen und Reports immer die gleichen Objekte der \ac{YARN}"=Komponenten zurückgegebenen
\end{itemize}

Da die REST"=API die Rückgabe der Daten in zwei Formaten ermöglicht, wurde der \texttt{RestParser} aufgrund der kleineren Datenmengen und übersichtlicheren Datenformats zur Nutzung des JSON"=Formats entwickelt
Einige Beispiele für die entsprechenden Pfade der REST"=API sowie deren Ausgaben im JSON"=Format sind in \cref{app:hadoopRestApi} zu finden.

Die Auswertung der Daten geschieht beim \texttt{RestParser} zudem nicht mit Regex, sondern mithilfe des \emph{Json.NET}"=Frameworks\footnote{\url{https://www.newtonsoft.com/json}}.
Hierfür wurden neben den bestehenden Datenhaltungsklassen noch weitere Hilfsklassen entwickelt, welche das Ausgabeformat der REST"=API nachbilden.
Mit deren Hilfe ist es möglich, alle Daten automatisch mithilfe von Json.NET aus den JSON"=Daten deserialisieren zu können.

Analog zum \texttt{CmdParser} müssen auch bei der Nutzung der REST"=API die Zeitstempel gesondert betrachtet werden.
Damit die Konvertierung der Java"=Zeitstempel in Millisekunden seit dem 1. Januar 1970 00:00:00 Uhr gemeinsam mit den anderen Daten durch das Json.NET"=Framework durchgeführt werden kann, musste hierfür ein gesonderter Konverter entwickelt werden.
Der Konverter nutzt ebenfalls die in \cref{lst:parseJavaTimestamp} gezeigte \texttt{ParseJavaTimestamp()} zum Konvertieren der Daten:

\begin{lstlisting}[label=lst:javaEpochConverter,style=cs,
caption={[Entwickelter Konverter für Java"=Zeitstempel zur Nutzung mit Json.NET]
    Entwickelter Konverter für Java"=Zeitstempel zur Nutzung mit Json.NET.
    Dieser erbt dafür von \texttt{DateTimeConverterBase} des Json.NET"=Frameworks, damit der \texttt{JsonJavaEpochConverter} auch zur Deserialisierung genutzt werden kann.}]
public class JsonJavaEpochConverter : DateTimeConverterBase
{
  private static readonly DateTime _Epoch = new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc);

  public override void WriteJson(JsonWriter writer, object value, JsonSerializer serializer)
  {
    writer.WriteRawValue(((DateTime)value - _Epoch).TotalMilliseconds.ToString());
  }
  
  public override object ReadJson(JsonReader reader, Type objectType, object existingValue, JsonSerializer serializer)
  {
    return DriverUtilities.ParseJavaTimestamp((long)reader.Value);
  }
}
\end{lstlisting}

Da der \ac{TLS} bei der Ausgabe der Daten zu Attempts und Containern zusätzliche Informationen enthält, werden hier nicht nur die Daten mithilfe des \ac{RM} bzw. der \ac{NM} der ausführenden Nodes, sondern auch mithilfe des \ac{TLS}.
Hierzu werden zunächst die Daten des \ac{RM} bzw. der \ac{NM} ermittelt und mit die zusätzlichen Informationen des \ac{TLS} ergänzt, sofern hier Daten verfügbar sind.
Aufgrund der Besonderheiten der REST"=API im Bezug auf Attempts und Container, werden bei diesen beiden \ac{YARN}"=Komponenten zunächst immer die Daten aller Attempts einer Anwendung bzw. aller Container eines Attempts ermittelt und konvertiert.
Sollten jedoch nur die Daten jeweils eines Attempts bzw. Containers benötigt werden, werden diese Listen entsprechend gefiltert:

\begin{lstlisting}[label=lst:restParseDetails,style=cs,
caption={[Konvertierung und Rückgabe der Daten eines einzelnen Containers durch den RestParser]
    Konvertierung und Rückgabe der Daten eines einzelnen Containers durch den \texttt{RestParser}.
    Hierbei muss zunächst die ID des übergeordneten Attempts ermittelt werden, bevor aus der Liste aller Container die Daten des gesuchten Containers zurückgegeben werden können.
    Bei Attempts ist dieses Vorgehen analog.}]
public IContainerResult ParseContainerDetails(string containerId)
{
  var attemptId = DriverUtilities.ConvertId(containerId, EConvertType.Attempt);
  var allContainers = ParseContainerList(attemptId);
  return allContainers.FirstOrDefault(c => c.ContainerId == containerId);
}
\end{lstlisting}

Aufgrund dieser Besonderheiten eignet sich auch das in \cref{subsubsec:yarnComponentInterface} beschriebene Monitoring durch die übergeordnete Komponente bei der Nutzung der REST"=API besser als das Selbstmonitoring.

\subsection{Entwicklung der Connectoren}
\label{subsec:implementedConnectors}

Der Connector dient zur Abstrahierung der SSH"=Verbindung (vgl. \cref{subsec:sshConnection}), damit diese in höheren Schichten einfach genutzt werden kann.
Dazu beinhaltet der Connector die jeweiligen Befehle, die auf dem Cluster"=Host ausgeführt werden.
Das Interface \texttt{IHadoopConnector} stellt Befehle für folgende Anwendungszwecke bereit:

\begin{itemize}
    \item Monitoring aller YARN"=Komponenten vom \ac{RM}, \ac{NM} oder \ac{TLS}
    \item Starten und Beenden von Nodes bzw. derer Netzwerkverbindungen
    \item Starten und Beenden von Anwendungen
    \item Prüfen und Löschen von Daten auf dem \ac{HDFS}
    \item Starten und Beenden des gesamten Clusters
    \item Monitoring des \ac{MARP}"=Wertes
\end{itemize}

Die implementierten Connectoren selbst bauen hierzu eine oder mehrere SSH"=Verbindungen auf, von denen jede Verbindung nur für einen bestimmten Typ an Befehlen genutzt wird.
Je nach Fähigkeiten des Connectors und Bedarf werden dadurch einzelne Verbindungen zum Monitoring, zum Injizieren und Reparieren von Komponentenfehlern oder zum Starten von Anwendungen aufgebaut.
Wenn das Cluster auf mehreren Hosts ausgeführt wird, werden zu jedem Host die entsprechend benötigten Verbindungen aufgebaut.
Um die SSH"=Verbindungen aufbauen zu können, nutzt der Connector die in \texttt{ModelSettings} gespeicherten Zugangsdaten der Hosts.

Das Initialisieren und Ausführen von Befehlen auf dem Cluster"=Host erfolgt ähnlich wie bei den Parsern immer nach dem gleichen Schema.
Zunächst werden beim Initialisieren des Connectors alle für seine Aufgaben benötigten SSH"=Verbindungen initialisiert und aufgebaut.
Wenn anschließend ein Befehl auf dem Cluster"=Host ausgeführt werden soll, wird zunächst geprüft, ob der Connector diesen Befehl unterstützt bzw. die dafür benötigte Verbindungen initialisiert wurden.
Ist das nicht der Fall, wird eine \texttt{PlatformNotSupportedException} ausgelöst, wenn der Connector den Befehl nicht unterstützt, bzw. eine  \texttt{InvalidOperationException} ausgelöst, wenn die benötigten Verbindungen nicht initialisiert wurden.
Anschließend werden die benötigten Parameter des auszuführenden Befehls ermittelt, zu denen auch die Auswahl des Hosts dazu gehört, auf dem der Befehl ausgeführt werden soll.
Nach der im Anschluss folgenden Ausführung des Befehls auf dem Host des Clusters werden die zurückgegebenen Daten bei Monitoring"=Befehlen im Rohformat an die anfragende Komponente weitergegeben und bei anderen Befehlen zunächst einer einfachen Auswertung unterzogen, damit das Ergebnis des Befehls durch die anfragende Komponente verarbeitet werden kann.
Aus diesem Grund werden die Befehle synchron ausgeführt, was bedeutet, dass bei der Ausführung eines Befehls immer bis zu seinem Ende gewartet wird.
Eine Ausnahme bildet hierbei das Starten von Anwendungen, was auch teilweise asynchron und vollständig asynchron durchgeführt werden kann.
Teilweise Asynchron bedeutet hier, dass die auf dem Cluster zu startende Anwendung nur solange synchron ausgeführt wird, bis der Anwendung eine ID zugewiesen wurde, welche zurückgegeben wird.
Diese Funktion wird daher zum Starten der Anwendungen durch den Client genutzt, da dieser die Anwendungs"=ID abspeichert (vgl. \cref{subsec:yarnClient}).

Da die beiden implementierten Parser unterschiedliche Befehle zum Abrufen der Daten benötigen, wurden hierfür zwei entsprechende Connectoren entwickelt.
Die Connectoren führen dabei nicht nur Befehle zum Monitoring aus, sondern alle die für ihre Schnittstellen verfügbaren und benötigten Befehle.

\subsubsection{Implementierung des CmdConnectors}
\label{subsubsec:implCmdConnector}

Der erste der beiden implementierten Connectoren ist der \texttt{CmdConnector}.
Er dient zum Ausführen von allen CLI"=Befehlen, die im Rahmen der Fallstudie benötigt werden.
Dabei wird nicht immer direkt mit Hadoop oder den Anwendungen interagiert, sondern immer mithilfe eines entsprechenden Setup"= oder Startscriptes (vgl. \todo{Setupscript}), welches vom Connector hierzu aufgerufen wird.
Das konkret genutzte Setupscript ist hierbei abhängig vom \texttt{HostMode}.
Da die Verwaltung des genutzten \texttt{HostMode}s vollständig durch die \texttt{ModelSettings} durchgeführt wird, kann der Connector unabhängig hiervon genutzt werden.
Notwendig ist hierfür jedoch, dass die Scripte die gleiche Befehlssyntax anbieten.

Da das Starten der Anwendungen die einzige asynchrone Operation darstellt, die der Connector durchführen muss, werden hierfür die meisten SSH"=Verbindung aufgebaut.
Wenn eine Anwendung gestartet werden soll, wird immer eine freie SSH"=Verbindung ausgewählt und die Anwendung mithilfe dieser gestartet.
Die Verbindung wird dabei solange zum Starten von anderen Anwendungen gesperrt, solange der ausführende Befehl, also die gestartete Anwendung, nicht beendet wurde (vgl. \cref{sec:sshDriver}).

\subsubsection{Implementierung des RestConnectors}
\label{subsubsec:implRestConnector}

Der zweite implementierte Connector, der \texttt{RestConnector} dient vor allem dem Monitoring mithilfe der REST"=API.
Aus diesem Grund lösen alle Methoden, die nicht dem Monitoring durch die REST"=API dienen, eine entsprechende \texttt{PlatformNotSupportedException} aus.

Zum Abrufen der Daten dienen ebenfalls SSH"=Verbindungen, jedoch ist hier eine pro Host, auf dem das Cluster ausgeführt wird, ausreichend.
Auf dem Host wird zum Abrufen der Daten das Tool curl\footnote{\url{https://curl.haxx.se/}} genutzt, wobei die Daten immer explizit im JSON"=Format angefragt werden.

Da die Daten von Anwendungs"=Containern immer durch die \ac{NM} der ausführenden Nodes zurückgegeben werden, kann es hier passieren, dass der entsprechende Node aufgrund eines Komponentenfehlers beendet oder nicht erreichbar ist.
Wenn daher statt den Container"=Daten eine Fehlermeldung zurückgegeben wird, werden in so einem Fall keine Rohdaten vom Connector zurückgegeben, sondern \texttt{String.Empty}.

Auch die Funktionen des \texttt{RestConnector}s können unabhängig vom genutzten \texttt{HostMode} genutzt werden.
Die Verwaltung der korrekten Adressen von \ac{RM} und \ac{TLS} erfolgt analog zum benötigten Setupscript zum \texttt{CmdConnector} durch die \texttt{ModelSettings}"=Klasse.
Ein Unterschied besteht jedoch bei den Adressen der \ac{NM} zum Monitoring der Container.
Da die Adressen der jeweiligen Nodes in der von \texttt{YarnHost} vererbten Eigenschaft \texttt{HttpUrl} gespeichert sind, werden die Adressen auch vom Connector genutzt, um mithilfe der REST"=API die Daten der auf einem Node ausgeführten Container zu abzurufen.
Daher wird die vom \texttt{HostMode} abhängige Adresse der Nodes beim Initialisieren des Modells entsprechend im Modell gespeichert.

\subsection{Implementierung der SSH"=Verbindung}
\label{subsec:sshConnection}

Die SSH"=Verbindung zum Host des realen Clusters wird mithilfe des Frameworks SSH.NET\footnote{\url{https://github.com/sshnet/SSH.NET}} aufgebaut.
Verwaltet wird die Verbindung mithilfe der Verbindungsklasse \texttt{SshConnection}.
Die Verbindung ist zudem der einzige Bestandteil des Treibers, welcher kein entsprechendes Interface bereitstellt, da die SSH"=Verbindungen ausschließlich durch die implementierten Connectoren genutzt werden.

Die zum Aufbau der Verbindung benötigten Zugangsdaten, die in \texttt{ModelSettings} gespeichert sind, müssen mithilfe des Connectors beim Initialisieren übergeben werden.
Hierbei stellt die Verbindungsklasse selbst zwar die Möglichkeit bereit, ein Passwort zu nutzen, jedoch besteht in \texttt{ModelSettings} nur die Angabe eines SSH"=Schlüssels zur Verfügung, was zudem die Sicherheit erhöht.
Zudem besteht bei der Nutzung eines SSH"=Schlüssels die Möglichkeit, die Verbindung jederzeit zu trennen und erneut aufzubauen, was im implementierten Modell jedoch nicht durchgeführt wird.

Die Verbindungsklasse ist auch dafür zuständig, die vom Connector erhaltenen auszuführenden Befehle an den Host zur Ausführung zu senden und auf die Rückgabe der Befehlsausgabe zu warten.
Hierbei wird zudem jede Kommunikation zwischen dem Treiber und dem Cluster"=Host in einer eigenen, von den Tests unabhängigen, Logdatei gespeichert (SSH"=Log).
Dabei ist die Verbindungsklasse auch dafür zuständig, dass beim teilsynchronen Starten einer Anwendung die Anwendungs"=ID ermittelt und die Anwendung anschließend asynchron ausgeführt wird.

Pro SSH"=Verbindung kann nur jeweils ein Befehl gleichzeitig auf dem Host ausgeführt werden.
Daher muss die Verbindungsklasse sicherstellen, dass die Verbindung auch bei asynchron ausgeführten Befehlen solange als belegt markiert ist, solange der Befehl nicht beendet ist.
Dies geschieht mithilfe einer entsprechenden Eigenschaft \texttt{InUse}, mit der freie und belegte Verbindungen voneinander getrennt werden können.
