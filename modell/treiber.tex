\section{Entwicklung des Treibers}
\label{sec:sshDriver}

In \cref{sec:modelArchitecture} wurde bereits aufgezeigt, dass der Treiber zur Verbindung des YARN"=Modells mit dem realen Cluster aus den drei Komponenten Parser, Connector und der SSH"=Verbindung selbst besteht.
Der Treiber ist im YARN"=Modell mithilfe verschiedener Interfaces zur Nutzung des Parsers und Connectors eingebunden.
Da YARN mithilfe von Befehlen für die \gls{CLI} und einer REST"=API zwei unterschiedliche Schnittstellen zum Auslesen der Daten der YARN"=Komponenten für das Monitoring bereitstellt, wurden jeweils zwei entsprechende Parser und Connectoren hierfür entwickelt.
Andere Befehle, wie \zB HDFS"=Befehle, können ebenfalls mithilfe des entwickelten \gls{CLI}"=Connectors ausgeführt werden, da Connectoren mithilfe von SSH"=Verbindungen mit den Cluster"=Hosts verbunden sind.

\subsection{Grundlegender Aufbau und Integration im YARN"=Modell}
\label{subsec:driverModelIntegration}

Zur Integration des Treibers im YARN"=Modell stellt dieser mehrere Interfaces bereit.
Dadurch sind einerseits der Treiber und das YARN"=Modell strikt getrennt, andererseits wird es dadurch auch ermöglicht, in Zukunft andere Möglichkeiten als die hier entwickelten zur Interaktion mit dem realen Cluster zu entwickeln und zu nutzen.

Zur Interaktion des YARN"=Modells mit dem Treiber werden dem Modell folgende Interfaces zur Verfügung gestellt:

\begin{itemize}
    \item \texttt{IHadoopParser} für Parser
    \item \texttt{IHadoopConnector} für Connectoren
    \item Von \texttt{IParsedComponent} abgeleitete Interfaces für geparste YARN"=Komponenten:
    \begin{itemize}
        \item \texttt{IApplicationResult} für Anwendungen
        \item \texttt{IAppAttemptResult} für Attempts
        \item \texttt{IContainerResult} für YARN"=Container
        \item \texttt{INodeResult} für Nodes
    \end{itemize}
\end{itemize}

Das Monitoring der Daten des realen Clusters wird mithilfe des Parsers durchgeführt.
Das Interface \texttt{IHadoopParser} stellt hierfür entsprechende Parsing"=Methoden für die vier implementierten YARN"=Komponenten sowie der Übersichtslisten aller einer YARN"=Komponente untergeordneten Subkomponenten.
Zudem stellt das Parser"=Interface eine Methode zum Auslesen des aktuellen \gls{MARP}"=Wertes des Schedulers bereit.
Beim Monitoring werden immer die entsprechenden von \texttt{IParsedComponent} abgeleiteten Interfaces zur Rückgabe der ermittelten Daten genutzt.
Hierfür stellen diese Interfaces entsprechende Eigenschaften bereit, um alle mithilfe der \gls{CLI} oder der REST"=API auslesbaren Daten an das YARN"=Modell übergeben zu können.

Das Connector"=Interface \texttt{IHadoopConnector} stellt alle zum Abrufen der Daten oder weiteren Interaktion, wie das Injizieren von Komponentenfehlern oder Starten von Anwendungen, benötigten Methoden und Befehle bereit.
Hierbei wird für das Monitoring unterschieden, ob die Daten vom \gls{TLS} oder vom \gls{RM} von Hadoop abgerufen werden.
Dies ist vor allem bei der Nutzung der REST"=API wichtig, da sich hier die Adressen und Pfade unterscheiden, während bei der Benutzung der \gls{CLI} die Befehle gleich sind.
Der \gls{TLS} wird zum Abrufen der Daten vor allem aus dem Grund genutzt, da hierbei zusätzliche Daten ermittelt werden können, die bei der reinen Nutzung der Schnittstellen des \gls{RM} nicht ausgegeben werden würden.
Ausgenommen sind hiervon Anwendungen, bei denen die Nutzung des \gls{TLS} keine weiteren Daten von Hadoop zurückgegeben werden\cite{HadoopYarnTlServer271,HadoopYarnCmds271,HadoopRmApi271,HadoopNmApi271}.
Aus diesem Grund ist die Nutzung des \gls{TLS} zum Monitoring von Anwendungen mithilfe des Connector"=Interfaces nicht möglich.

Die implementierten Parser und Connectoren sind jeweils als Singleton realisiert und werden bei der Ausführung des YARN"=Modells durch den \gls{ss}"=Simulator weitestgehend ignoriert.
Dies ist vor allem darin begründet, dass dadurch Speicher eingespart wird, der dadurch für andere YARN"=Komponenten zur Verfügung steht.
Aber auch weitere Einschränkungen durch \gls{ss} spielten eine Rolle (vgl. \cref{sec:ssharp}).
Ein weiterer Vorteil liegt zudem darin, dass für unterschiedliche Einsatzzwecke der gleiche Connector genutzt werden kann, und somit auch die einzelnen vom Connector benötigten SSH"=Verbindungen für unterschiedliche Einsatzzwecke wiederverwendet werden können.
Die Initialisierung der Parser und Connectoren erfolgt jeweils beim ersten Aufruf der Singletons.
Dies geschieht innerhalb des Testsystems üblicherweise entweder beim Initialisieren des Tests (vgl. \cref{sec:implTestcases}) oder beim Initialisieren des YARN"=Modells selbst durch die Klasse \texttt{Model}.
Die Initialisierung des Modells stellt zudem den einzigen Zeitpunkt dar, bei dem im YARN"=Modell direkt mit den implementierten Parsern und Connectoren interagiert wird, jede andere Interaktion findet stattdessen gekapselt mithilfe der Interfaces statt.

Die drei Komponenten des Treibers sind zudem untereinander  voneinander gekapselt.
Bei der Ausführung des Parsers wird daher analog ebenfalls nur zur Initialisierung mit dem konkreten Connector interagiert, während zum Monitoring das Connector"=Interface als einzige Schnittstelle dient.
Da für die SSH"=Verbindung kein eigenes Interface zur Kapselung existiert, ist die Kapselung der Connectoren und der bereitstellenden Klasse der SSH"=Verbindung nicht so streng wie zwischen anderen Komponenten.
Dennoch werden Befehle auf den Cluster"=Hosts ausschließlich mithilfe des Connectors durchgeführt.

\subsection{Entwicklung der Parser}
\label{subsec:implementedParsers}

Der Parser dient dazu, die von Hadoop zurückgegebenen Daten der YARN"=Komponenten zu konvertieren, sodass sie im Modell gespeichert werden können.
Zur Datenhaltung innerhalb der Parser"=Komponente des Treibers wurden hierfür entsprechende Klassen entwickelt, welche die von \texttt{IParsedComponent} abgeleiteten Interfaces implementieren.
Dadurch sind die Datenhaltungs"=Klassen dafür geeignet, die Daten an das Modell zu übergeben.
Eine Besonderheit bilden hierbei die ausführenden Nodes der Container bzw. \gls{AppMstr}"=Container.
Während bei den anderen YARN"=Komponenten lediglich die jeweiligen IDs zurückgegeben werden, werden bei den ausführenden Nodes direkt ihre korrespondierenden Instanzen im Modell zurückgegeben.

Der grundlegende Ablauf der Initialisierung und dem Abrufen und Konvertieren der Daten ist bei beiden implementierten Parsern gleich.
Beim Initialisieren des Parsers wird zunächst der benötigte, passende Connector initialisiert, sofern dieser noch nicht initialisiert wurde.
Beim Abrufen und Konvertieren der Daten werden zunächst die je nach Komponente benötigten IDs von weiteren YARN"=Komponenten ermittelt, bevor die Rohdaten durch den Connector ermittelt werden.
Die Rohdaten werden anschließend konvertiert und in der entsprechenden Datenhaltungs"=Klasse gespeichert, welche mithilfe der entsprechenden von \texttt{IParsedComponent} abgeleiteten Interfaces an das Modell zurückgegeben werden.

Obwohl in dieser Fallstudie das Monitoring ausschließlich mithilfe der wesentlich schnelleren REST"=API durchgeführt wird, wurde auch ein Parser zur Nutzung der \gls{CLI}"=Schnittstelle entwickelt.
Dies liegt darin begründet, dass Hadoop diese beiden Schnittstellen zum Monitoring der Anwendungen zur Verfügung stellt.

\subsubsection{Implementierung des CmdParsers}
\label{subsubsec:implCmdParser}

Der erste der beiden entwickelten Parser ist der \texttt{CmdParser} zum Monitoring mithilfe von \gls{CLI}"=Befehlen.
Zum Auslesen der Daten selbst nutzt der \texttt{CmdParser} den dazugehörigen \texttt{CmdConnector} (vgl. \cref{subsubsec:implCmdConnector}), mit dem zum Auslesen der Daten die entsprechenden \gls{CLI}"=Befehle ausgeführt werden.
Die hierbei zurückgegebenen Daten sind im Vergleich zu den von der REST"=API zurückgegebenen im Umfang deutlich reduziert, in den jeweiligen Übersichten der Subkomponenten auf das notwendigste beschränkt.
Im Gegenzug zur REST"=API werden hier die Daten des \gls{RM} und \gls{TLS} kombiniert ausgegeben.
Eine kurze Übersicht über einige Befehle und deren Ausgaben ist im \cref{app:hadoopCmds} zu finden.

Ausgewertet werden die von Hadoop zurückgegebenen Daten mithilfe von \glspl{Regex}.
Da das Ausgabeformat jeweils in Listenform oder als ausführlicher Report immer das gleiche Format aufweist, wurden hierfür zwei generische Regex"=Pattern entwickelt, welche zur Auswertung fast aller Daten ausreichend sind:

\begin{lstlisting}[label=lst:cmdRegexPattern,style=cs,
caption={[Implementierte \glsentryshort{Regex}"=Pattern des CmdParsers]
    Implementierte \acrshort{Regex}"=Pattern des \texttt{CmdParser}s}]
Regex _GenericListRegex = new Regex(@"\s*([^\t]+)");
Regex _GenericDetailsRegex =
   new Regex(@"\t(.+)\s:\s([^\t]*)[\n\r]", RegexOptions.Multiline);
\end{lstlisting}

Bei zurückgegebenen Listen müssen diese zur Auswertung zunächst zeilenweise getrennt werden, was bei einzelnen Reports nicht nötig ist.
Dies wurde aus dem Grund so umgesetzt, da in Listen jede zurückgegebene Zeile eine eigene Komponente darstellt, wogegen ein Report aus mehreren Zeilen besteht.
Danach können durch die Reihenfolge der jeweiligen Regex"=Matchgruppen die jeweiligen Daten der Komponente den entsprechenden Eigenschaften der Datenhaltungsklassen zugeordnet werden.

Eine Besonderheit bildet das Konvertieren der Zeitstempel.
Diese werden von Hadoop meist in Form der Millisekunden seit dem 1. Januar 1970 00:00:00 Uhr, der Java"=Epoche, zurückgegeben \cite{HadoopRmApi271,HadoopNmApi271,HadoopYarnTlServer271,JavaInstantDoc}.
In der Liste der ausgeführten Container eines Attempts wird der Zeitstempel stattdessen im Format \texttt{ddd MMM dd HH:mm:ss zz00 yyyy} ausgegeben, was \zB dem Zeitstempel \texttt{Fri Jan 05 11:08:16 +0000 2018} entspricht.
Weiterführende Informationen zur Formatierung von Zeitstempeln in .NET finden sich in \cite{CsTimeFormatStrings}, zur Zeitberechnung in Java \uA in \cite{JavaInstantDoc}.
Der von Hadoop zurückgegebene Zeitstempel muss in beiden Fällen zunächst in ein .NET"=kompatibles Format umgewandelt werden.
Dies geschieht mithilfe der Methode \texttt{ParseJavaTimestamp()}, für die mehrere Überladungen implementiert wurden:

\begin{lstlisting}[label=lst:parseJavaTimestamp,style=cs,
caption={[Überladungen der Methode ParseJavaTimestamp()]
    Überladungen der Methode \texttt{ParseJavaTimestamp()}.
    Es steht zudem eine weitere Überladung zur Verfügung, um den Timestamp in Form der Millisekunden seit 1970 als \texttt{string} zu übergeben.
    Dabei wird der \texttt{string} in einen \texttt{long} konvertiert und anschließend die erste hier gezeigte Überladung aufgerufen.}]
public static DateTime ParseJavaTimestamp(long javaMillis)
{
  if(javaMillis < 1)
  return DateTime.MinValue;
  var javaTimeUtc = new DateTime(1970, 1, 1, 0, 0, 0,
     DateTimeKind.Utc).AddMilliseconds(javaMillis);
  return javaTimeUtc.ToLocalTime();
}

public static DateTime ParseJavaTimestamp(string value,
   string format, CultureInfo culture = null)
{
  culture = culture ?? new CultureInfo("en-US");
  DateTime time;
  DateTime.TryParseExact(value, format, culture,
     DateTimeStyles.AssumeUniversal, out time);
  return time;
}
\end{lstlisting}

Die hierbei zurückgegebenen \texttt{DateTime}"=Instanzen werden anschließend zum Speichern der Zeitstempel genutzt.

Die Speicherung und Übergabe der ausführenden Nodes des \gls{AppMstr} oder der Container an das Modell geschieht direkt als entsprechende Node"=Instanz innerhalb des Modells.
Hierfür wird die ID bzw. die URL des Nodes genutzt, um die korrespondierende Instanz im YARN"=Modell zu ermitteln und zu speichern.

\subsubsection{Implementierung des RestParsers}
\label{subsubsec:implRestParser}

Der zweite entwickelte Parser ist der \texttt{RestParser}.
Er dient dazu, um die mithilfe der REST"=API zurückgegebenen Daten auszuwerten und an das YARN"=Modell zu übergeben.
Zum Auslesen der Daten aus dem Cluster wurde hierfür der \texttt{RestConnector} entwickelt (vgl. \cref{subsubsec:implRestConnector}).
Die REST"=API besitzt, auch im Vergleich zur \gls{CLI}"=Schnittstelle, einige Besonderheiten, auf die bei der Implementierung geachtet werden musste \cite{HadoopYarnCmds271,HadoopRmApi271,HadoopNmApi271,HadoopYarnTlServer271}:

\begin{itemize}
    \item Die zurückgegebenen Daten sind deutlich umfangreicher als bei der \gls{CLI}"=Schnittstelle
    \item Die Daten können im XML- oder JSON"=Format zurückgegebenen werden
    \item Attempts können nur als Liste aller Attempts einer Anwendung zurückgegeben werden
    \item Daten zu Containern können nur durch die \gls{NM} der ausführenden Nodes ermittelt werden
    \item Die Adressen und Pfade von \gls{RM}, \gls{NM} und \gls{TLS} unterscheiden sich
    \item Die Daten von \gls{RM} und \gls{NM} sind immer umfangreicher als die des \gls{TLS}
    \item Der \gls{TLS} enthält für Attempts und YARN"=Container jedoch zusätzliche Daten
    \item Es werden bei Listen und Reports immer die gleichen Objekte der YARN"=Komponenten zurückgegebenen
\end{itemize}

Da die REST"=API zwei Ausgabeformate unterstützt, wurde der \texttt{RestParser} aufgrund der kleineren Datenmengen und des übersichtlicheren Datenformats zur Nutzung des JSON"=Formats entwickelt.
Einige Beispiele für die entsprechenden Pfade der REST"=API sowie deren Ausgaben im JSON"=Format sind in \cref{app:hadoopRestApi} zu finden.

Die Auswertung der Daten wird beim \texttt{RestParser} nicht mit Regex, sondern mithilfe des \emph{Json.NET}"=Frameworks\footnote{\url{https://www.newtonsoft.com/json/}} durchgeführt.
Hierfür wurden neben den bestehenden Datenhaltungsklassen noch weitere Hilfsklassen entwickelt, welche das Ausgabeformat der REST"=API nachbilden.
Mit deren Hilfe ist es möglich, die Daten mithilfe von Json.NET konvertieren zu können.

Analog zum \texttt{CmdParser} müssen auch bei der Nutzung der REST"=API die Zeitstempel gesondert betrachtet werden.
Damit die Konvertierung der Java"=Zeitstempel in Millisekunden seit dem 1. Januar 1970 00:00:00 Uhr gemeinsam mit den anderen Daten durch das Json.NET"=Framework durchgeführt werden kann, musste hierfür ein gesonderter Konverter entwickelt werden.
Der Konverter nutzt ebenfalls die in \cref{lst:parseJavaTimestamp} gezeigte \texttt{ParseJavaTimestamp()} zum Konvertieren der Daten:

\begin{lstlisting}[label=lst:javaEpochConverter,style=cs,
caption={[Entwickelter Konverter für Java"=Zeitstempel zur Nutzung mit Json.NET]
    Entwickelter Konverter für Java"=Zeitstempel zur Nutzung mit Json.NET.
    Dieser erbt dafür von \texttt{DateTimeConverterBase} des Json.NET"=Frameworks, damit der \texttt{JsonJavaEpochConverter} auch zur Konvertierung genutzt werden kann.}]
public class JsonJavaEpochConverter : DateTimeConverterBase
{
  private static readonly DateTime _Epoch =
     new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc);

  public override void WriteJson(JsonWriter writer,
     object value, JsonSerializer serializer)
  {
    writer.WriteRawValue(
       ((DateTime)value - _Epoch).TotalMilliseconds.ToString());
  }
  
  public override object ReadJson(JsonReader reader, Type objectType,
     object existingValue, JsonSerializer serializer)
  {
    return DriverUtilities.ParseJavaTimestamp((long)reader.Value);
  }
}
\end{lstlisting}

Da der \gls{TLS} zusätzliche Informationen zu Attempts und Containern enthält, werden hier nicht nur die Daten mithilfe des \gls{RM} bzw. der \gls{NM} der ausführenden Nodes, sondern auch mithilfe des \gls{TLS} ermittelt.
Hierzu werden zunächst die Daten des \gls{RM} bzw. der \gls{NM} ermittelt und mit den zusätzlichen Informationen des \gls{TLS} ergänzt, sofern hier Daten verfügbar sind.
Aufgrund der Besonderheiten der REST"=API in Bezug auf Attempts und Container, werden bei diesen beiden YARN"=Komponenten zunächst immer die Daten aller Attempts einer Anwendung bzw. aller Container eines Attempts ermittelt und konvertiert.
Sollten jedoch nur die Daten jeweils eines Attempts bzw. Containers benötigt werden, werden diese Listen entsprechend gefiltert:

\begin{lstlisting}[label=lst:restParseDetails,style=cs,
caption={[Konvertierung und Rückgabe eines Containers durch den RestParser]
    Konvertierung und Rückgabe eines Containers durch den \texttt{RestParser}.
    Hierbei muss für den hier gezeigten, einzelnen Container zunächst die ID des übergeordneten Attempts ermittelt werden, bevor aus der Liste aller Container die Daten des gesuchten Containers zurückgegeben werden können.
    Bei Attempts ist dieses Vorgehen analog.}]
public IContainerResult ParseContainerDetails(string containerId)
{
  var attemptId = DriverUtilities.ConvertId(containerId, EConvertType.Attempt);
  var allContainers = ParseContainerList(attemptId);
  return allContainers.FirstOrDefault(c => c.ContainerId == containerId);
}
\end{lstlisting}

Aufgrund dieser Besonderheiten eignet sich auch das in \cref{subsec:yarnComponentInterface} beschriebene Monitoring durch die übergeordnete Komponente bei der Nutzung der REST"=API besser als das Selbstmonitoring.

\subsection{Entwicklung der Connectoren}
\label{subsec:implementedConnectors}

Der Connector dient zur Abstrahierung der SSH"=Verbindung (vgl. \cref{subsec:sshConnection}), damit diese in höheren Schichten einfach genutzt werden kann.
Dazu beinhaltet der Connector die jeweiligen Befehle, die auf dem Cluster"=Host ausgeführt werden können:

\begin{itemize}
    \item Monitoring aller YARN"=Komponenten vom \gls{RM}, \gls{NM} oder \gls{TLS}
    \item Starten und Beenden von Nodes bzw. derer Netzwerkverbindungen
    \item Starten und Beenden von Anwendungen
    \item Prüfen und Löschen von Daten auf dem HDFS
    \item Starten und Beenden des gesamten Clusters
    \item Monitoring des \gls{MARP}"=Wertes
\end{itemize}

Die implementierten Connectoren stellen hierzu eine oder mehrere SSH"=Verbindungen her, von denen jede Verbindung nur für einen bestimmten Typ an Befehlen genutzt wird.
Je nach Fähigkeiten des Connectors werden dadurch einzelne Verbindungen zum Monitoring, zum Injizieren und Reparieren von Komponentenfehlern oder zum Starten von Anwendungen aufgebaut.
Wenn das Cluster auf mehreren Hosts ausgeführt wird, werden zu jedem Host die entsprechend benötigten Verbindungen aufgebaut.

Das Initialisieren und Ausführen von Befehlen auf dem Cluster"=Host erfolgt ähnlich wie bei den Parsern immer nach dem gleichen Schema.
Zunächst werden beim Initialisieren des Connectors alle für seine Aufgaben benötigten SSH"=Verbindungen initialisiert und aufgebaut.
Die hierfür notwendigen Zugangsdaten werden durch den Connector aus den \texttt{ModelSettings} den Verbindungen zur Verfügung gestellt (vgl. \cref{subsec:modelClass,subsec:sshConnection}).
Bevor ein Befehl auf dem Cluster"=Host ausgeführt wird, wird zunächst geprüft, ob der Connector diesen Befehl unterstützt bzw. die dafür benötigten Verbindungen initialisiert wurden.
Wenn der Connector den Befehl nicht unterstützt, wird stattdessen eine \texttt{PlatformNotSupportedException} ausgelöst, wenn die benötigten Verbindungen nicht initialisiert wurden eine \texttt{InvalidOperationException}.
Anschließend werden die benötigten Parameter des auszuführenden Befehls ermittelt, zu denen auch die Auswahl des Hosts dazu gehört, auf dem der Befehl ausgeführt werden soll.
Nach der im Anschluss folgenden Ausführung des Befehls auf dem Host des Clusters werden die zurückgegebenen Daten bei Monitoring"=Befehlen im Rohformat an die anfragende Komponente weitergeleitet bzw. bei anderen Befehlen zunächst einer einfachen Auswertung unterzogen, damit das Ergebnis des Befehls durch die anfragende Komponente verarbeitet werden kann.
Aus diesem Grund werden die meisten Befehle synchron ausgeführt, wodurch die weitere Ausführung eines Testfalls solange unterbrochen wird, solange der Befehl auf dem Host ausgeführt wird.
Eine Ausnahme bildet hierbei das Starten von Anwendungen, was auch \emph{teilsynchron} und vollständig asynchron durchgeführt werden kann.
Teilsynchron bedeutet hier, dass die auf dem Cluster zu startende Anwendung solange synchron ausgeführt wird, bis der Anwendung eine ID zugewiesen wurde, die vom Connector zurückgegeben wird, bevor sie anschließend asynchron ausgeführt wird.
Diese Funktion wird daher zum Starten der Anwendungen durch den Client genutzt, da dieser die Anwendungs"=ID abspeichert (vgl. \cref{subsec:yarnClient}).

Da die beiden implementierten Parser unterschiedliche Befehle zum Abrufen der Daten benötigen, wurden hierfür zwei entsprechende Connectoren entwickelt.
Die Connectoren führen dabei nicht nur Befehle zum Monitoring aus, sondern alle die für ihre Schnittstellen verfügbaren und benötigten Befehle.
Dies wirkt sich vor allem auf den \texttt{CmdConnector} aus, da dieser in dieser Fallstudie bis auf das Monitoring jede Aktion mit dem Cluster durchführt.

\subsubsection{Implementierung des CmdConnectors}
\label{subsubsec:implCmdConnector}

Der erste der beiden implementierten Connectoren ist der \texttt{CmdConnector}.
Er dient zum Ausführen von allen CLI"=Befehlen, die im Rahmen der Fallstudie benötigt werden.
Dabei wird nicht immer direkt mit Hadoop oder den Anwendungen interagiert, sondern immer mithilfe eines entsprechenden Setup"= oder Startscriptes (vgl. \cref{subsec:scripts}), welches vom Connector hierzu aufgerufen wird.
Das konkret genutzte Setup"=Script ist hierbei abhängig vom \texttt{HostMode}.
Da die Verwaltung des genutzten \texttt{HostMode}s vollständig durch die \texttt{ModelSettings} durchgeführt wird, kann der Connector unabhängig hiervon genutzt werden (vgl. \cref{subsec:modelClass}).
Notwendig ist hierfür jedoch, dass die Scripte immer die in \cref{app:setupScriptCmds} gezeigten Befehle enthalten.

Da das Starten der Anwendungen die einzige asynchrone Operation darstellt, die der Connector durchführen muss, werden hierfür die meisten SSH"=Verbindung aufgebaut.
Wenn eine Anwendung gestartet werden soll, wird immer eine freie SSH"=Verbindung ausgewählt und die Anwendung mithilfe dieser gestartet.
Die Verbindung wird dabei solange zum Starten von anderen Anwendungen gesperrt, solange der ausführende Befehl, also die gestartete Anwendung, nicht beendet wurde (vgl. \cref{sec:sshDriver}).
Sind dadurch alle Submitter belegt und eine weitere Anwendung soll gestartet werden, wird nach einer gewissen Zeit eine \texttt{TimeoutException} ausgelöst.
Die Befehlsparameter zum Starten der Anwendungen werden von den jeweiligen Benchmarks zur Verfügung gestellt, sodass der Connector nur das Benchmark"=Script mit den bereitgestellten Befehlsparametern ausführen muss (vgl. \cref{subsec:scripts,subsec:appImplementation}).

\subsubsection{Implementierung des RestConnectors}
\label{subsubsec:implRestConnector}

Der zweite implementierte Connector, der \texttt{RestConnector}, dient vor allem dem Monitoring mithilfe der REST"=API.
Aus diesem Grund lösen alle Methoden, die nicht dem Monitoring durch die REST"=API dienen, eine entsprechende \texttt{PlatformNotSupported""Exception} aus.

Zum Abrufen der Daten dienen ebenfalls SSH"=Verbindungen, jedoch ist hier eine pro Host, auf dem das Cluster ausgeführt wird, ausreichend.
Auf dem Host wird zum Abrufen der Daten das Tool curl\footnote{\url{https://curl.haxx.se/}} genutzt, wobei die Daten immer explizit im JSON"=Format angefragt werden.

Da die Daten von YARN"=Containern immer durch die \gls{NM} der ausführenden Nodes zurückgegeben werden, kann es hier passieren, dass der entsprechende Node aufgrund eines Komponentenfehlers beendet oder nicht erreichbar ist.
Wenn daher statt den Container"=Daten eine Fehlermeldung zurückgegeben wird, werden in so einem Fall keine Rohdaten vom Connector zurückgegeben, sondern \texttt{String.Empty}.

Auch die grundlegenden Funktionen des \texttt{RestConnector}s sind unabhängig vom genutzten \texttt{HostMode} (vgl. \cref{subsec:hostMode}).
Die Verwaltung der Adressen von \gls{RM} und \gls{TLS} erfolgt analog zu der des Setup"=Scripts beim \texttt{CmdConnector} durch die \texttt{ModelSettings}"=Klasse.
Ein Unterschied besteht jedoch bei den Adressen der \gls{NM} zum Monitoring der Container.
Da die Adressen der jeweiligen Nodes in \texttt{YarnNode.HttpUrl} gespeichert sind, werden diese Adressen auch vom Connector genutzt, um mithilfe der REST"=API die Daten der auf einem Node ausgeführten Container zu abzurufen.
Daher wird die vom \texttt{HostMode} abhängige Adresse der Nodes bereits beim Initialisieren des Modells entsprechend im Modell gespeichert (vgl. \cref{subsec:modelClass,subsec:yarnComponents}).

\subsection{Implementierung der SSH"=Verbindung}
\label{subsec:sshConnection}

Die SSH"=Verbindung zum Host des realen Clusters wird mithilfe des Frameworks SSH.NET\footnote{\url{https://github.com/sshnet/SSH.NET}} hergestellt.
Verwaltet werden die Verbindungen mithilfe der Klasse \texttt{SshCon""nection}.
Die Verbindung ist zudem der einzige Bestandteil des Treibers, zu dem kein entsprechendes Interface existiert, da die SSH"=Verbindungen ausschließlich durch die implementierten Connectoren genutzt werden.

Die in \texttt{ModelSettings} gespeicherten Zugangsdaten zum Aufbau der Verbindung benötigten Zugangsdaten müssen beim Initialisieren vom Connector übergeben werden \cref{subsec:modelClass}).
Zwar kann hierzu mithilfe der Verbindungsklasse auch ein Passwort genutzt werden, jedoch werden in \texttt{ModelSettings} nur die SSH"=Schlüssel gespeichert.
Zudem bietet die implementierte \texttt{SshConnection}"=Klasse bei der Nutzung eines SSH"=Schlüssels die Möglichkeit, die Verbindung jederzeit zu trennen und erneut aufzubauen, was im implementierten Modell jedoch nicht durchgeführt wird.

Die Verbindungsklasse ist auch dafür zuständig, die vom Connector erhaltenen auszuführenden Befehle synchron an den Host zur Ausführung zu senden.
Hierbei wird zudem jede Kommunikation zwischen dem Treiber und dem Cluster"=Host in einer eigenen, vom Testsystem unabhängigen, Logdatei gespeichert, dem SSH"=Log.
Die Verbindungsklasse ist auch dafür zuständig, dass beim teilsynchronen Starten einer Anwendung zunächst die Anwendungs"=ID ermittelt wird, bevor die Anwendung asynchron ausgeführt wird (vgl. \cref{subsubsec:implCmdConnector,subsec:yarnClient}).

Je SSH"=Verbindung kann nur ein Befehl gleichzeitig auf dem Host ausgeführt werden.
Daher muss die Verbindungsklasse sicherstellen, dass die Verbindung auch bei asynchron ausgeführten Befehlen solange als belegt markiert ist, solange der Befehl nicht beendet ist.
Dies geschieht mithilfe einer entsprechenden Eigenschaft \texttt{InUse}, mit der freie und belegte Verbindungen voneinander getrennt werden können.
