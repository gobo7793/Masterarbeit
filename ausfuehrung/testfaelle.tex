\section{Auswahl der Testkonfigurationen}
\label{sec:selectTestcases}

Die im in \cref{subsec:testcaseGeneration} beschriebenen Testkonfigurationen werden mithilfe verschiedener Variablen implementiert.
Anhand dieser Konfiguration wurden dynamisch zur Laufzeit die entsprechenden Testfälle generiert.

Relevant für die Ausführung einer Konfigurationen sind folgende, bereits in \cref{lst:hadoopSimulationInit} gezeigte, Eigenschaften:

\begin{lstlisting}[label=lst:hadoopTest,style=cs,
caption={Zur Definition einer Testkonfiguration relevante Felder}]
public int BenchmarkSeed { get; set; } = Environment.TickCount;
public double FaultActivationProbability { get; set; } = 0.25;
public double FaultRepairProbability { get; set; } = 0.5;
public int HostsCount { get; set; } = 1;
public int NodeBaseCount { get; set; } = 4;
public int ClientCount { get; set; } = 2;
\end{lstlisting}

Da die jeweiligen Auswirkungen der Eigenschaften bereits in \cref{subsec:simulationModelInit} erläutert wurden, wird an dieser Stelle hierauf verweisen.

Zur Festlegung dieser Variablen und damit der Testkonfigurationen wurde zunächst eine Systematik entwickelt, nach welcher die Testfälle durchgeführt werden.
Hierfür wurden mithilfe des folgenden Programmcodes zunächst zwei Seeds ermittelt:

\begin{lstlisting}[label=lst:generateTestCaseSeeds,style=cs,
caption={Ermittlung der für die Testkonfigurationen genutzten Basisseeds}]
public void GenerateCaseStudyBenchSeeds()
{
  var ticks = Environment.TickCount;
  var ran = new Random(ticks);
  var s1 = ran.Next(0, int.MaxValue);
  var s2 = ran.Next(0, int.MaxValue);
  Console.WriteLine($"Ticks: 0x{ticks:X}");
  Console.WriteLine($"s1: 0x{s1:X} | s2: 0x{s2:X}");
  // Specific output f§§or generating test c§§ase seeds:
  // Ticks: 0x5829F2
  // s1: 0xAB4FEDD | s2: 0x11399D3
}
\end{lstlisting}

Die beiden ermittelten Seeds 0xAB4FEDD und 0x11399D3 stellen somit die erste Variable einer Testkonfiguration dar.

Zur Festlegung der Werte zur generellen Wahrscheinlichkeiten zur Aktivierung bzw. Deaktivierung von Komponentenfehlern wurden über 20.000 mögliche Aktivierungen und Deaktivierungen mit verschiedenen generellen Wahrscheinlichkeiten und Auslastungsgraden der Nodes simuliert.
Der dabei für alle Testkonfigurationen ausgewählte Wert von 0,3 stellt hierbei eine ausgewogene Aktivierung bzw. Deaktivierung der Komponentenfehler bei unterschiedlichen Auslastungsgraden der Nodes dar.

Die Anzahl der Hosts wurde bei einigen Konfigurationen auf 1 festgelegt, bei den meisten liegt diese jedoch bei 2.
Die Node"=Basisanzahl wurde bei allen Konfigurationen auf 4 festgelegt, da hierbei das Cluster eine ausreichende Größe (4 oder 6 Nodes, vgl. \cref{subsec:hostMode,subsec:simulationModelInit}) besitzt und jedem Node ausreichend Ressourcen zur Verfügung stehen, um Anwendungen auszuführen.
Bei einer zu hohen Basisanzahl erhält jeder einzelne Node geringere Ressourcen, was vor allem die Ausführung bei ressourcenintensiven Anwendungen wie \zB \acrlong{pt} behindert, während bei einer zu geringen das Cluster sehr klein ist und daher keine ausreichende Evaluationsbasis bietet.
Die Anzahl der auszuführenden Testfälle wurde variiert, wodurch in einigen Konfigurationen 5 und in anderen 10 Testfälle auszuführende Testfälle definiert sind.
Ebenso variiert wurde die Anzahl der simulierten Clients, die im Bereich von 2, 4 oder 6 Clients liegt.

Alle Konfigurationen wurden mindestens einmal jeweils mit der Selfbalancing"=Komponente ohne Mutationen sowie in einem der in \cref{sec:implMutationTests} erläuterten Mutationsszenarien ausgeführt.
Von den hiermit möglichen 48 Testkonfigurationen wurden die möglichen Konfigurationen mit einem Host und sechs simulierten Clients sowie die möglichen Konfigurationen mit zwei simulierten Clients und zehn Testfällen nicht ausgeführt.
Das ergibt für die Evaluation somit eine Datenbasis von 32 grundlegenden Testkonfigurationen.
Eine Übersicht aller genutzten Testkonfigurationen, der ausgeführten Tests mit ihrer benötigten Ausführungszeit, sowie der Anzahl der dabei tatsächlich ausgeführten Testfälle, ist in \cref{app:overviewExecutedTestCases} zu finden.

Bei der Ausführung der Tests zur Evaluation wurden Eingabedaten nicht vorab generiert, sondern während der Ausführung von den Anwendungen direkt generiert (vgl. \cref{sec:clusterSetup,subsec:testcaseGeneration,subsec:precreateInputData,subsec:simulationModelInit}).
Dies liegt darin begründet, da durch die Vorabgenerierung der \gls{MARP}"=Wert manipuliert werden würde, wodurch die Tests an Aussagekraft verlieren.

Dazu wurde die Mindestdauer für einen Testfall bei allen Konfigurationen auf 25 Sekunden festgelegt, da in diesem Zeitraum die meisten \emph{kleinen} Anwendungen erfolgreich durchgeführt werden können, wenn sonst keine Anwendung aktiv ist.
Eine ausreichende Mindestdauer ist vor allem für die Generierung der Eingabedaten für nachfolgende Anwendungen wichtig, da nicht vollständig generierte Daten von abgebrochenen Anwendungen nicht von nachfolgenden Anwendungen genutzt werden können.
Zudem stellt dies eine ausreichende Zeitspanne zur Rekonfiguration von Hadoop dar.
