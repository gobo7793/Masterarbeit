\section{Implementierung der Simulation}
\label{sec:implSimulation}

Alle zur Ausführung der Simulation relevanten Methoden sind in der Klasse \texttt{Simulation""Tests} zusammengefasst.
Hierbei gibt es neben der Simulation mit Aktivierung der Komponentenfehler auch eine Möglichkeit zur Simulation ohne die Aktivierung von Komponentenfehlern sowie weitere, mit der Simulation zusammenhängende Methoden.

\subsection{Grundlegender Aufbau}
\label{subsec:simulationBasics}

Da im realen Hadoop"=Cluster kontinuierlich Anpassungen und \gls{ss}"=Tests dagegen mit diskreten Schritten durchgeführt werden, muss beachtet werden, dass die bei den Tests ermittelten Werte nur Momentaufnahmen darstellen.
Ebenso muss beachtet werden, dass bei der Deaktivierung von einzelnen Nodes bzw. deren Netzwerkverbindungen diese nicht in Echtzeit, sondern um einige Zeit verzögert erkannt und erst nach einer gewissen Zeit aus der Konfiguration des Clusters entfernt werden (vgl. \cref{sec:hadoop}).
Genauso verhält es sich, wenn ein Node bzw. seine Verbindung wieder aktiviert wird, da dieser zunächst gestartet und anschließend die Verbindung mit den \gls{RM} hergestellt werden muss.
Außerdem werden die für die auf dem Cluster ausgeführten Anwendungen benötigten \gls{AppMstr} und YARN"=Container aufgrund der komplexen internen Prozesse von Hadoop nicht innerhalb weniger Millisekunden allokiert, sondern benötigen ebenfalls eine gewisse Zeit.
Aus diesen Gründen muss ein vom Simulator ausgeführter Testfall um eine gewisse Zeit verzögert werden, sodass alle Aktivitäten zur Verwaltung des Hadoop"=Clusters genügend Zeit zur Ausführung erhalten.
Dadurch erhält jeder Testfall eine gewisse Mindestdauer, die erreicht sein muss, bevor der nachfolgende Testfall ausgeführt werden kann.

Beim grundlegenden Ablauf der Simulation wird dieser Umstand daher bereits berücksichtigt:

\begin{lstlisting}[label=lst:hadoopSimulation,style=cs,
caption={[Ausführung der Simulation]
    Ausführung der Simulation (gekürzt).}]
private bool ExecuteSimulation()
{
  var model = InitModel();
  // 0.000001 to prevent inaccuracy
  var isWithFaults = FaultActivationProbability > 0.000001;
  
  var wasFatalError = false;
  try
  {
    // init simulation
    var simulator = new SafetySharpSimulator(model);
    var simModel = (Model)simulator.Model;
    var faults = CollectYarnNodeFaults(simModel);
    
    SimulateBenchmarks();
    
    // d§§o simuluation
    for(var i = 0; i < StepCount; i++)
    {
      OutputUtilities.PrintStepStart();
      var stepStartTime = DateTime.Now;
      
      if(isWithFaults)
      HandleFaults(faults);
      simulator.SimulateStep(); // execute test c§§ase
      
      var stepTime = DateTime.Now - stepStartTime;
      OutputUtilities.PrintDuration(stepTime);
      if(stepTime < ModelSettings.MinStepTime)
      Thread.Sleep(ModelSettings.MinStepTime - stepTime);
      
      OutputUtilities.PrintFullTrace(simModel.Controller);
    }
    // collect fault counts and check constraint
  }
  // cat§§ch/fin§§ally
  
  return !wasFatalError;
}
\end{lstlisting}

Hierbei gibt es zwei Variationen zum Ausführen der Simulation, welche abhängig von der Aktivierung der Komponentenfehler ist.
Sollen keine Komponentenfehler aktiviert bzw. deaktiviert werden, werden die Variablen zur Festlegung der generellen Wahrscheinlichkeiten (vgl. \cref{subsec:simulationModelInit}) entsprechend gesetzt und die Simulation ausgeführt.
Nach Abschluss eines Testfalls durch \texttt{SimulateStep()} wird geprüft, ob die Ausführung des Testfalls die Mindestdauer unterschritten hat und die weitere Ausführung in dem Fall entsprechend verzögert.

Wenn während der Simulation eine im Modell nicht behandelte \texttt{Exception} auftritt, wird diese außerhalb der Simulation abgefangen und entsprechend geloggt.
Dadurch wird zudem die Simulation beim aktuellen Stand abgebrochen.
Nach Abschluss der Simulation werden alle noch ausgeführten Anwendungen beendet und defekte Nodes neu gestartet, sofern nötig.

Für die Simulation selbst sind zudem ebenfalls Constraints definiert.
Dies ist dadurch nötig, da die Anforderung, dass Komponentenfehler injiziert bzw. repariert werden (vgl. \cref{subsec:testRequirements}), nicht immer innerhalb eines Testfalls validiert werden kann.
Aus diesem Grund wird diese Anforderung in Form von Constraints nach Abschluss einer Simulation durch das Oracle geprüft.

\subsection{Initialisierung des Modells}
\label{subsec:simulationModelInit}

Bevor das Modell im Simulator ausgeführt werden kann, muss es initialisiert werden.
Das folgende \cref{lst:hadoopSimulationInit} zeigt die Definition der zur Initialisierung benötigten Eigenschaften sowie die Methode \texttt{InitModel()}, die in \cref{lst:hadoopSimulation} zur Initialisierung des YARN"=Modells aufgerufen wird:

\begin{lstlisting}[label=lst:hadoopSimulationInit,style=cs,
caption={Initialisierung des Modells für die Simulation}]
public TimeSpan MinStepTime { get; set; } = new TimeSpan(0, 0, 0, 25);
public int BenchmarkSeed { get; set; } = Environment.TickCount;
public int StepCount { get; set; } = 3;
public bool PrecreatedInputs { get; set; } = true;
public bool RecreatePreInputs { get; set; } = false;
public double FaultActivationProbability { get; set; } = 0.25;
public double FaultRepairProbability { get; set; } = 0.5;
public int HostsCount { get; set; } = 1;
public int NodeBaseCount { get; set; } = 4;
public int ClientCount { get; set; } = 2;

private Model InitModel()
{
  ModelSettings.HostMode = ModelSettings.EHostMode.Multihost;
  // save fields i§§n ModelSettings
  
  var model = Model.Instance;
  model.InitModel(appCount: StepCount, clientCount: ClientCount);
  model.Faults.SuppressActivations();
  
  return model;
}
\end{lstlisting}

Die Werte der Eigenschaften werden vor der Initialisierung des Modells in den \texttt{ModelSettings} gespeichert.
Die dort gespeicherten Werte werden wiederum zum Initialisieren der Modell"=Instanz bzw. während der Ausführung der Simulation genutzt (vgl. \cref{subsec:modelClass}).

Einige Eigenschaften haben lediglich einen Zweck, während andere umfangreichere Auswirkungen besitzen.
Die einfachen Eigenschaften sind:

\begin{description}
    \item [MinStepTime] \hfill \\
        Definiert die Mindestdauer eines Schrittes.
        
    \item[BenchmarkSeed] \hfill \\
        Gibt den Seed an, mit dem die Zufallsgeneratoren in den Klassen \texttt{Benchmark""Controller} und \texttt{NodeFaultAttribute} initialisiert werden.
        Dadurch wird es ermöglicht, einzelne Testfälle erneut ausführen zu können.
        
    \item[StepCount] \hfill \\
        Definiert die Anzahl der auszuführenden Testfälle der Simulation.
        
    \item[FaultActivationProbability] \hfill \\
        Definiert die generelle Wahrscheinlichkeit zum Aktivieren von Komponentenfehlern.
        Ist dieser Wert 0,0, werden grundsätzlich keine Komponentenfehler aktiviert, bei einem Wert von 1,0 werden Komponentenfehler dagegen immer aktiviert.
        
    \item[FaultRepairProbability] \hfill \\
        Definiert die generelle Wahrscheinlichkeit zum Deaktivieren von Komponentenfehlern.
        Die hier definierte Wahrscheinlichkeit verhält sich analog zu \texttt{FaultActi""vation""Probability}.
        Bei einem Wert von 0,0 werden Komponentenfehler niemals deaktiviert, während sie bei einem Wert von 1,0 im nachfolgenden Schritt immer deaktiviert werden.
        
    \item[HostsCount] \hfill \\
        Definiert die Anzahl der in der Simulation verwendeten Hosts.
        Benötigt wird dieser Wert, damit zu jedem verwendeten Host eine SSH"=Verbindung aufgebaut werden kann (vgl. \cref{subsec:implementedConnectors}).
        Diese Eigenschaft hat jedoch nur einen Einfluss, wenn der \texttt{Mutlihost}"=Mode genutzt wird (vgl. \cref{subsec:hostMode}).
        
    \item[NodeBaseCount] \hfill \\
        Definiert die Anzahl der Nodes auf Host1.
        Auf anderen Hosts wird jeweils die Hälfte der Nodes verwendet (vgl. \cref{subsec:hostMode}).
        Benötigt wird dieser Wert, um mithilfe der REST"=API auf die Hadoop"=Nodes zugreifen zu können, um die Daten der YARN"=Container zu ermitteln (vgl. \cref{subsubsec:implRestConnector,subsec:modelClass}).
        
    \item[ClientCount] \hfill \\
        Definiert die Anzahl der zu simulierenden Clients.
        Da jeder Client gleichzeitig nur eine Anwendung startet, wird dadurch gleichzeitig definiert, wie viele Anwendungen gleichzeitig auf dem Cluster ausgeführt werden können.
\end{description}

Eine Besonderheit bildet die Eigenschaft \texttt{PrecreatedInputs}.
Sie definiert, ob die ausgeführten Anwendungen auf dem Cluster vorab generierte Eingabedaten nutzen oder alle Eingabedaten während der Ausführung selbst generieren.
Der Unterschied zwischen beiden Varianten liegt darin, dass vorab generierte Eingabedaten in einem anderen Verzeichnis im HDFS gespeichert sind und während der Simulation die Eingabedaten aus diesem Verzeichnis gelesen werden.
Wenn keine Eingabedaten vorab generiert werden, werden als Eingabeverzeichnisse für die Anwendungen die Ausgabeverzeichnisse der entsprechenden Benchmarks genutzt, welche die dafür benötigten Daten generieren (vgl. \cref{sec:clusterSetup,subsec:testcaseGeneration,subsec:precreateInputData}).
Die Eigenschaft \texttt{RecreatePreInputs} definiert hierfür, ob bereits bestehende Eingabedaten neu generiert werden, was standardmäßig nicht der Fall ist bzw. die Eigenschaft auf \texttt{false} gesetzt ist (vgl. \cref{subsec:precreateInputData}).
Die Werte der beiden Eigenschaften werden daher entsprechend in ihren korrespondierenden Eigenschaften in \texttt{ModelSettings} gespeichert.

Die direkt im Anschluss an die Initialisierung des Simulators aufgerufene Methode \texttt{CollectYarnNodeFaults()} ermittelt alle im initialisierten Modell enthaltenen, mit \texttt{NodeFaultAttribute} markierten Komponentenfehler (vgl. \cref{subsec:yarnComponentFaults}):

\begin{lstlisting}[label=lst:hadoopSimulationCollectFaults,style=cs,
caption={[Ermitteln der Komponentenfehler mit dem NodeFaultAttribute]
    Ermitteln der Komponentenfehler mit dem \texttt{NodeFaultAttribute}}]
private FaultTuple[] CollectYarnNodeFaults(Model model)
{
  return (from node in model.Nodes      
    from faultField in node.GetType().GetFields()
    where typeof(Fault).IsAssignableFrom(faultField.FieldType)
    
    let attribute =
       faultField.GetCustomAttribute<NodeFaultAttribute>()
    where attribute != null
    
    let fault = (Fault)faultField.GetValue(node)
    
    select Tuple.Create(fault, attribute, node,
       new IntWrapper(0), new IntWrapper(0))
  ).ToArray();
}
\end{lstlisting}

Die gefundenen Komponentenfehler werden als Tupel"=Array, bestehend aus dem Komponentenfehler selbst, dem Attribut sowie dem dazugehörigen Node zurückgegeben.
Zur Speicherung hierfür dient der Typ \texttt{FaultTuple}, welcher ein Alias für das hierfür genutzte \texttt{Tupel<T>} darstellt.
Die jeweiligen Instanzen der Attribute und Nodes werden für die in \cref{subsec:faultActivation} beschriebene Aktivierung der dazugehörigen Komponentenfehler benötigt.
Die beiden im Tupel gespeicherten Instanzen des \texttt{IntWrapper} dienen zur Speicherung der Anzahl der Aktivierungen bzw. Deaktivierungen der Komponentenfehler.
Da der Wert eines Structs wie \texttt{int} nicht direkt in einem Tupel geändert werden kann, dient die Klasse \texttt{IntWrapper} hierfür als Adapter.
Benötigt werden diese Werte zur Validierung der Constraints der Simulation durch das Oracle (vgl. \cref{subsec:simulationBasics}).

\subsection{Weitere mit der Simulation zusammenhängende Methoden}
\label{subsec:simulationUtilities}

Neben der Ausführung der Simulation mit und ohne der Möglichkeit zur Aktivierung der Komponentenfehler gibt es noch einige weitere Methoden, die mit der Simulation zusammenhängen.
So kann \zB die Vorabgenerierung der Eingabedaten durch eine entsprechende Methode durchgeführt werden (vgl. \cref{subsec:precreateInputData}).
Es ist aber auch möglich, nur die Simulation der durch den Benchmark"=Controller ausgewählten Benchmarks durchzuführen.
Hierzu kann die bei der Ausführung der Simulation aufgerufene Methode \texttt{SimulateBenchmark()} als eigener Test durchgeführt werden:

\begin{lstlisting}[label=lst:hadoopSimulationBenchmarks,style=cs,
caption={Simulation der auszuführenden Benchmarks}]
[Test]
public void SimulateBenchmarks()
{
  for(int i = 1; i <= _ClientCount; i++)
  {
    var seed = _BenchmarkSeed + i;
    var benchController = new BenchmarkController(seed);
    Logger.Info($"Simulating Benchmarks f§§or " +
       "Client {i} with Seed {seed}:");
    for(int j = 0; j < _StepCount; j++)
    {
      benchController.ChangeBenchmark();
      Logger.Info($"Step {j}: " +
         "{benchController.CurrentBenchmark.Name}");
    }
  }
}
\end{lstlisting}

\subsection{Ablauf eines Tests und der Testfälle}
\label{subsec:simulationStep}

Zu Beginn eines Tests werden zunächst die in \cref{subsec:dataOrganisation} definierten, grundlegenden Daten des Tests im Programmlog gespeichert.
Daneben werden aber auch noch weitere, auch den \texttt{HostMode} (vgl. \cref{subsec:hostMode}) betreffende, Daten im Programmlog abgespeichert:

\begin{itemize}
    \item Verbundene SSH"=Verbindungen mit einer ID zur besseren Zuordnung im SSH"=Log
    \item Ausführung der Erstellung von vorab generierten Eingabedaten
    \item Vollständiger Pfad des verwendeten Setup"=Scriptes
    \item Adresse des Controllers zur Nutzung der REST"=API
    \item Simulation der auszuführenden Benchmarks (vgl. \cref{subsec:simulationUtilities})
\end{itemize}

Im Anschluss werden das auszuführende YARN"=Modell und der zur Ausführung des Tests genutzte Simulator initialisiert, bevor die Testfälle selbst ausgeführt werden.

Der Ablauf eines Testfalls lässt sich in mehrere Abschnitte einteilen.
Zunächst wird vom Simulator mithilfe der Attribute der Komponentenfehler entschieden, ob ein Komponentenfehler aktiviert und im Cluster injiziert wird (vgl. \cref{subsec:yarnComponentFaults,subsec:faultActivation}).
Anschließend führt der Simulator den Testfall aus, indem für alle Komponenten des Modells die \texttt{Update()}"=Methoden ausgeführt werden.
Hierdurch werden deaktivierte Komponentenfehler auch im Cluster wieder repariert, da \texttt{YarnNode.Update()} die entsprechenden Start"=Methoden aufruft (vgl. \cref{subsec:yarnComponentFaults}).

Anschließend wird die in \cref{subsec:yarnController} erläuterte Routine des Controllers ausgeführt.
Dabei wird zunächst der \gls{MARP}"=Wert aus dem Cluster ausgelesen, bevor die Benchmark"=Anwendungen des Testfalls gestartet werden.
Jeder Client nutzt dafür die in \cref{subsec:yarnClient} beschriebe Routine, um zunächst durch den Benchmark"=Controller die zu startende Anwendung auszuwählen (vgl. \cref{subsec:selectionNextBenchmark}) und zu starten.
Die dabei vom Cluster zugewiesene Anwendungs"=ID wird vom Client gespeichert, um die Anwendung in nachfolgenden Testfällen bei Bedarf abbrechen zu können, wenn eine neue Anwendung gestartet werden soll (vgl. \cref{subsec:yarnClient}).

Sobald alle Anwendungen gestartet sind, wird vom Controller das Monitoring aller Nodes, Anwendungen und ihrer Attempts durchgeführt.
Dabei wird der \gls{MARP}"=Wert erneut ausgelesen, da sich der Wert durch die Selfbalancing"=Komponente regelmäßig ändern kann (vgl. \cref{subsec:yarnController,sec:inriaSetting}).

Den Abschluss eines Testfalls bildet die Validierung der Werte des Clusters, die beim Monitoring im YARN"=Modell gespeichert wurden.
Dazu werden für jede Komponente im Modell die jeweiligen auf den Anforderungen basierenden Constraints durch das Oracle geprüft (vgl. \cref{sec:requirements,subsec:yarnComponentConstraints,subsec:yarnController}).
Wenn ein Constraint nicht erfolgreich validiert wurde, wird dies jeweils im Programmlog gespeichert bzw. die Ausführung der Simulation abgebrochen, wenn sich das Cluster nicht mehr rekonfigurieren kann (vgl. \cref{subsec:oracleImpl}).

Nach Abschluss eines Testfalls werden durch den Simulator die in \cref{subsec:dataOrganisation} geforderten Daten im Programmlog abgespeichert, die beim Monitoring erkannt wurden.
Daneben werden bei der Ausführung eines Testfalls aber auch weitere Daten im Programmlog gespeichert, wie \zB:

\begin{itemize}
    \item Ausführung von Komponentenfehlern
    \item Diagnostik"=Daten der YARN"=Komponenten
    \item Verletzte Constraints und die betroffenen YARN"=Komponenten
    \item Die Information, wenn eine Rekonfiguration nicht möglich ist
\end{itemize}

Zum Abschluss der Simulation werden zudem die für die Simulation als gesamtes betreffenden Constraints validiert, welche nicht im YARN"=Modell selbst implementiert wurden (vgl. \cref{subsec:simulationBasics}).

Nach Abschluss der Simulation wird ein erneutes Monitoring des gesamten Clusters durchgeführt.
Der hierbei ermittelte Status wird im Programmlog als finaler Clusterstatus und gemeinsam mit einigen statistischen Kenndaten gespeichert:

\begin{itemize}
    \item Gesamtdauer der Simulation
    \item Anzahl erfolgreicher Testfälle
    \item Anzahl der maximal möglichen, aktivierbaren Komponentenfehler
    \item Anzahl aktivierter und deaktivierter Komponentenfehler
    \item Letzter ermittelter \gls{MARP}"=Wert
    \item Anzahl aller ausgeführten, erfolgreichen, fehlgeschlagenen sowie abgebrochenen Anwendungen
    \item Anzahl aller ausgeführten Attempts
    \item Anzahl aller während der Ausführung erkannten Container
    \item Anzahl aller validierten Constraints und verletzten Constraints, getrennt nach \gls{SuT}- und Testsystem"=Constraints
\end{itemize}

Der auszugsweise Programmlog eines Tests findet sich in \cref{app:outputFormat}.