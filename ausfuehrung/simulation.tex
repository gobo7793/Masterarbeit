\section{Implementierung der Simulation}
\label{sec:implSimulation}

Alle zur Ausführung der Simulation relevanten Methoden sind in der Klasse \texttt{SimulationTests} zusammengefasst.
Hierbei gibt es neben der Simulation mit Aktivierung der Komponentenfehler auch eine Möglichkeit zur Simulation ohne die Aktivierung von Komponentenfehlern sowie weitere, mit der Simulation zusammenhängende Methoden.

\subsection{Grundlegender Aufbau}
\label{subsec:simulationBasics}

Da im realen Cluster Hadoop kontinuierlich Anpassungen durchführt und \glspl{Test} in \gls{ss} mit diskreten Schritten durchgeführt werden, muss beachtet werden, dass die Werte, die beim \gls{Test} ermittelt werden, immer nur Momentaufnahmen darstellen.
Ebenso muss beachtet werden, dass bei der Deaktivierung von einzelnen Nodes bzw. deren Netzwerkverbindungen diese nicht in Echtzeit, sondern um einige Zeit verzögert erkannt werden und erst nach einer gewissen Zeit aus der Konfiguration des Clusters entfernt werden (vgl. \cref{sec:hadoop}).
Genauso verhält es sich, wenn ein Node bzw. seine Verbindung wieder aktiviert wird, da dieser zunächst gestartet und die Verbindung mit den YARN"=Controller wiederhergestellt werden muss.
Außerdem werden die für die auf dem Cluster ausgeführten \glspl{Anwendung} benötigten \gls{AppMstr} und YARN"=Container aufgrund der komplexen internen Prozesse von Hadoop nicht innerhalb weniger Millisekunden allokiert, sondern benötigen ebenfalls eine gewisse Zeit.
Aus diesen Gründen muss ein vom Simulator ausgeführter Testfall um eine gewisse Zeit verzögert werden, sodass alle Aktivitäten innerhalb von Hadoop genügend Zeit zur Ausführung erhalten.
Dadurch erhält jeder Testfall eine gewisse Mindestdauer, die erreicht sein muss, bevor der nachfolgende Testfall ausgeführt wird.

Der grundlegende Ablauf einer Simulation sieht wie folgt aus:

\begin{lstlisting}[label=lst:hadoopSimulation,style=cs,
caption={[Durchführung der Simulation]
    Durchführung der Simulation (gekürzt).}]
private bool ExecuteSimulation()
{
  var model = InitModel();
  // 0.000001 to prevent inaccuracy
  var isWithFaults = FaultActivationProbability > 0.000001;
  
  var wasFatalError = false;
  try
  {
    // init simulation
    var simulator = new SafetySharpSimulator(model);
    var simModel = (Model)simulator.Model;
    var faults = CollectYarnNodeFaults(simModel);
    
    SimulateBenchmarks();
    
    // d§§o simuluation
    for(var i = 0; i < StepCount; i++)
    {
      OutputUtilities.PrintStepStart();
      var stepStartTime = DateTime.Now;
      
      if(isWithFaults)
      HandleFaults(faults);
      simulator.SimulateStep();
      
      var stepTime = DateTime.Now - stepStartTime;
      OutputUtilities.PrintDuration(stepTime);
      if(stepTime < ModelSettings.MinStepTime)
      Thread.Sleep(ModelSettings.MinStepTime - stepTime);
      
      OutputUtilities.PrintFullTrace(simModel.Controller);
    }
    
    // collect fault counts and check constraint
  }
  // cat§§ch/fin§§ally
  
  return !wasFatalError;
}
\end{lstlisting}

Hierbei gibt es zwei Varianten zum Ausführen der Simulation, welche abhängig von der Aktivierung der Komponentenfehler ist.
Sollen keine Komponentenfehler aktiviert bzw. deaktiviert werden, werden die entsprechenden Variablen zur Festlegung der generellen Wahrscheinlichkeiten (vgl. \cref{subsec:simulationModelInit} entsprechend gesetzt und die Simulation ausgeführt.
Nach Abschluss eines Testfalls durch \texttt{SimulateStep()} wird geprüft, ob die Ausführung des Testfalls die Mindestdauer unterschritten hat und die weitere Ausführung in dem Fall entsprechend verzögert.

Wenn während der Simulation eine im Modell nicht behandelte \texttt{Exception} auftritt, wird diese außerhalb der Simulation abgefangen und entsprechend geloggt.
Dadurch wird zudem die Simulation beim aktuellen Stand abgebrochen.
Nach Abschluss der Simulation werden alle noch ausgeführten \glspl{Anwendung} beendet und defekte Nodes neu gestartet, sofern nötig.

Für die Simulation selbst sind zudem ebenfalls Constraints definiert.
Dies ist dadurch nötig, da die in \cref{subsec:testRequirements} definierte Anforderung, dass Komponentenfehler injiziert bzw. repariert werden, nicht immer innerhalb eines Testfalls validiert werden können.
Aus diesem Grund wird diese Anforderung in Form von Constraints nach Abschluss einer Simulation durch das Oracle geprüft.

\subsection{Initialisierung des Modells}
\label{subsec:simulationModelInit}

Bevor das Modell im Simulator ausgeführt werden kann, muss es initialisiert werden.
Das folgende \cref{lst:hadoopSimulationInit} zeigt die Definition der Felder zur Modellinitialisierung sowie die entsprechenden Methoden, die in \cref{lst:hadoopSimulation} zur Initialisierung aufgerufen werden:

\begin{lstlisting}[label=lst:hadoopSimulationInit,style=cs,
caption={Initialisierung des Modells für die Simulation}]
public TimeSpan MinStepTime { get; set; } = new TimeSpan(0, 0, 0, 25);
public int BenchmarkSeed { get; set; } = Environment.TickCount;
public int StepCount { get; set; } = 3;
public bool PrecreatedInputs { get; set; } = true;
public bool RecreatePreInputs { get; set; } = false;
public double FaultActivationProbability { get; set; } = 0.25;
public double FaultRepairProbability { get; set; } = 0.5;
public int HostsCount { get; set; } = 1;
public int NodeBaseCount { get; set; } = 4;
public int ClientCount { get; set; } = 2;

private Model InitModel()
{
  ModelSettings.HostMode = ModelSettings.EHostMode.Multihost;
  ModelSettings.HostsCount = HostsCount;
  ModelSettings.NodeBaseCount = NodeBaseCount;
  ModelSettings.IsPrecreateBenchInputsRecreate = RecreatePreInputs;
  ModelSettings.IsPrecreateBenchInputs = PrecreatedInputs;
  ModelSettings.RandomBaseSeed = BenchmarkSeed;
  
  var model = Model.Instance;
  model.InitModel(appCount: StepCount, clientCount: ClientCount);
  model.Faults.SuppressActivations();
  
  return model;
}
\end{lstlisting}

Die einzelnen Eigenschaften für die Simulation werden vor dem Initialisieren des Modells in den \texttt{ModelSettings} gespeichert.
Die dort gespeicherten Werte werden wiederum zum Initialisieren der Modell"=Instanz bzw. während der Ausführung der Simulation genutzt.

Einige Eigenschaften haben lediglich einen Zweck, während andere umfangreichere Auswirkungen besitzen.
Die einfachen Eigenschaften sind:

\begin{description}
    \item [MinStepTime] \hfill \\
        Definiert die Mindestdauer eines Schrittes.
        
    \item[BenchmarkSeed] \hfill \\
        Gibt den Seed an, mit dem die Zufallsgeneratoren in den Klassen \texttt{Benchmark""Controller} und \texttt{NodeFaultAttribute} initialisiert werden.
        Dadurch wird es ermöglicht, einzelne \glspl{Testfall} erneut ausführen zu können.
        
    \item[StepCount] \hfill \\
        Definiert die Anzahl der ausgeführten Testfälle der Simulation.
        
    \item[FaultActivationProbability] \hfill \\
        Definiert die generelle Häufigkeit zum Aktivieren von Komponentenfehlern.
        Ist dieser Wert 0,0, werden grundsätzlich keine Komponentenfehler aktiviert, bei einem Wert von 1,0 werden Komponentenfehler dagegen immer aktiviert.
        
    \item[FaultRepariProbability] \hfill \\
        Definiert die generelle Häufigkeit zum Deaktivieren von Komponentenfehlern.
        Die hier definierte Wahrscheinlichkeit verhält sich analog zu \texttt{\_FaultActivation""Probability}.
        Bei einem Wert von 0,0 werden Komponentenfehler niemals deaktiviert, während sie bei einem Wert von 1,0 im nachfolgenden Schritt immer deaktiviert werden.
        
    \item[HostsCount] \hfill \\
        Definiert die Anzahl der in der Simulation verwendeten Hosts.
        Benötigt wird dieser Wert, damit zu jedem verwendeten Host eine SSH"=Verbindung aufgebaut werden kann (vgl. \cref{subsec:implementedConnectors}).
        Diese Eigenschaft hat jedoch nur einen Einfluss, wenn der \texttt{Mutlihost}"=Mode genutzt wird (vgl. \cref{subsec:hostMode}).
        
    \item[NodeBaseCount] \hfill \\
        Definiert die Anzahl der Nodes auf Host1.
        Auf einem möglichen Host2 wird die Hälfte der Nodes verwendet (vgl. \cref{subsec:hostMode}).
        Benötigt wird dieser Wert, um mithilfe der \gls{REST}"=API auf die Hadoop"=Nodes zugreifen zu können, um die Daten der YARN"=Container zu ermitteln (vgl. \cref{subsubsec:implRestConnector,subsec:modelClass}).
        
    \item[ClientCount] \hfill \\
        Definiert die Anzahl der zu simulierenden Clients.
        Da jeder Client gleichzeitig nur eine \gls{Anwendung} startet, wird dadurch gleichzeitig definiert, wie viele \glspl{Anwendung} gleichzeitig auf dem Cluster ausgeführt werden sollen.
\end{description}

Eine Besonderheit bildet die Eigenschaft \texttt{PrecreatedInputs}.
Es definiert, ob die ausgeführten \glspl{Anwendung} auf dem Cluster vorab generierte Eingabedaten nutzen oder alle Eingabedaten während der Ausführung selbst generieren.
Der Unterschied zwischen beiden Varianten liegt darin, dass vorab generierte Eingabedaten in einem anderen Verzeichnis im \gls{HDFS} gespeichert sind und während der Simulation die Eingabedaten aus diesem Verzeichnis gelesen werden.
Wenn keine Eingabedaten vorab generiert werden, werden als Eingabeverzeichnisse für die \glspl{Anwendung} die Ausgabeverzeichnisse der entsprechenden Benchmarks genutzt, die die dafür benötigten Daten generieren (vgl. \cref{sec:clusterSetup,subsec:testcaseGeneration,subsec:precreateInputData}).
Die Eigenschaft \texttt{RecreatePreInputs} definiert hierfür, ob bereits bestehende Eingabedaten neu generiert werden, was standardmäßig nicht der Fall ist bzw. dieses Feld auf \texttt{false} gesetzt ist (vgl. \cref{subsec:precreateInputData}).
Die Werte der beiden Eigenschaften werden daher entsprechend in ihren korrespondierenden Eigenschaften in \texttt{ModelSettings} gespeichert.

Die direkt im Anschluss an die Initialisierung des Simulators aufgerufene Methode \texttt{CollectYarnNodeFaults()} ermittelt alle im initialisierten Modell enthaltenen Komponentenfehler, die mit dem \texttt{NodeFaultAttribute} markiert sind (vgl. \cref{subsec:yarnComponentFaults}):

\begin{lstlisting}[label=lst:hadoopSimulationCollectFaults,style=cs,
caption={[Ermitteln der Komponentenfehler mit dem NodeFaultAttribute]
    Ermitteln der Komponentenfehler mit dem \texttt{NodeFaultAttribute}}]
private FaultTuple[] CollectYarnNodeFaults(Model model)
{
  return (from node in model.Nodes      
    from faultField in node.GetType().GetFields()
    where typeof(Fault).IsAssignableFrom(faultField.FieldType)
    
    let attribute = faultField.GetCustomAttribute<NodeFaultAttribute>()
    where attribute != null
    
    let fault = (Fault)faultField.GetValue(node)
    
    select Tuple.Create(fault, attribute, node, new IntWrapper(0), new IntWrapper(0))
  ).ToArray();
}
\end{lstlisting}

Die gefundenen Komponentenfehler werden als Array aus Tupel, bestehend aus dem Komponentenfehler selbst, dem Attribut sowie dem dazugehörigen Node zurückgegeben.
Zur Speicherung hierfür dient der Typ \texttt{FaultTuple}, welcher ein Alias für das hierfür genutzte \texttt{Tupel<T>} darstellt.
Die jeweiligen Instanzen der Attribute und Nodes werden für die in \cref{subsec:faultActivation} beschriebene Aktivierung der dazugehörigen Komponentenfehler benötigt.
Die beiden im Tupel gespeicherten Instanzen des \texttt{IntWrapper} dienen zur Speicherung der Anzahl der Aktivierungen bzw. Deaktivierungen der Komponentenfehler.
Da der Wert einer Struktur wie \texttt{int} nicht direkt in einem Tupel geändert werden kann, dient die Klasse \texttt{IntWrapper} hierfür als Adapter.
Benötigt werden diese Werte zur Validierung der Constraints der Simulation durch das Oracle (vgl. \cref{subsec:simulationBasics})

\subsection{Weitere mit der Simulation zusammenhängende Methoden}
\label{subsec:simulationUtilities}

Neben der Ausführung der Simulation mit und ohne der Möglichkeit zur Aktivierung der Komponentenfehler gibt es noch einige weitere Methoden, die mit der Simulation zusammenhängen.
So kann \zB die Vorabgenerierung der Eingabedaten durch eine entsprechende Methode durchgeführt werden (vgl. \cref{subsec:precreateInputData}).
Es ist aber auch möglich, nur die Simulation der durch den Benchmark"=Controller ausgewählten Benchmarks durchzuführen.
Hierzu kann die bei der Ausführung der Simulation aufgerufene Methode \texttt{SimulateBenchmark()} als eigener \gls{Test} durchgeführt werden:

\begin{lstlisting}[label=lst:hadoopSimulationBenchmarks,style=cs,
caption={Simulation der auszuführenden Benchmarks}]
[Test]
public void SimulateBenchmarks()
{
  for(int i = 1; i <= _ClientCount; i++)
  {
    var seed = _BenchmarkSeed + i;
    var benchController = new BenchmarkController(seed);
    Logger.Info($"Simulating Benchmarks for Client {i} with Seed {seed}:");
    for(int j = 0; j < _StepCount; j++)
    {
      benchController.ChangeBenchmark();
      Logger.Info($"Step {j}: {benchController.CurrentBenchmark.Name}");
    }
  }
}
\end{lstlisting}

\subsection{Ablauf eines Tests und der Testfälle}
\label{subsec:simulationStep}

Zu Beginn eines \glspl{Test} werden zunächst die in \cref{subsec:dataOrganisation} definierten, grundlegenden Daten des \glspl{Test} im Programmlog gespeichert.
Daneben werden aber auch noch weitere, auch den \texttt{HostMode} (vgl. \cref{subsec:hostMode}) betreffende Daten im Programmlog abgespeichert:

\begin{itemize}
    \item Verbundene SSH"=Verbindungen mit einer ID zur besseren Zuordnung im SSH"=Log
    \item Ausführung der Erstellung von vorab generierten Eingabedaten
    \item Vollständiger Pfad des verwendeten Setup"=Scriptes
    \item Adresse des Controllers zur Nutzung der \gls{REST}"=API
    \item Simulation der auszuführenden Benchmarks (vgl. \cref{subsec:simulationUtilities}
\end{itemize}

Im Anschluss werden das auszuführende \gls{YARN}"=Modell und der zur Ausführung des \glspl{Test} genutzte Simulator initialisiert, bevor die \glspl{Testfall} selbst ausgeführt werden.

Der Ablauf eines Testfalls lässt sich mehrere Abschnitte einteilen.
Zunächst wird vom Simulator selbst mithilfe der den Komponentenfehlern zugewiesenen Attribute geprüft bzw. entschieden, ob ein Komponentenfehler aktiviert und im Cluster injiziert wird (vgl. \cref{subsec:yarnComponentFaults,subsec:faultActivation}).
Anschließend führt der Simulator den Testfall aus, indem für alle Komponenten des Modells die \texttt{Update()}"=Methoden ausgeführt werden.
Hierdurch werden deaktivierte Komponentenfehler auch im Cluster wieder repariert, da \texttt{YarnNode.Update()} die entsprechenden Start"=Methoden aufruft (vgl. \cref{subsec:yarnComponentFaults}).

Anschließend wird die in \cref{subsec:yarnController} erläuterte Routine des Controllers ausgeführt.
Dabei wird zunächst der \gls{MARP}"=Wert aus dem Cluster ausgelesen, bevor die Benchmark"=Anwendungen des Testfalls gestartet werden.
Jeder Client nutzt dafür die in \cref{subsec:yarnClient} beschriebe Routine, um zunächst durch den Benchmark"=Controller die zu startende \gls{Anwendung} auszuwählen (vgl. \cref{subsec:selectionNextBenchmark}) und zu starten.
Die dabei vom Cluster zugewiesene Anwendungs"=ID wird vom Client gespeichert, um die \gls{Anwendung} in nachfolgenden Testfällen bei Bedarf abbrechen zu können, wenn eine neue \gls{Anwendung} gestartet werden soll (vgl. \cref{subsec:yarnClient}).

Sobald alle \glspl{Anwendung} gestartet sind, wird vom Controller das Monitoring aller Nodes, \glspl{Anwendung} und ihrer \glspl{Attempt} durchgeführt.
Dabei wird auch der \gls{MARP}"=Wert ein zweites mal ausgelesen, da er sich durch die zuvor noch aktiven bzw. neu gestarteten \glspl{Anwendung} unterschiedlich verändert haben könnte (vgl. \cref{subsec:yarnController}).

Den Abschluss eines Testfalls bildet die Validierung der Werte des Clusters, die beim Monitoring im \gls{YARN}"=Modell gespeichert wurden.
Dazu werden für jede Komponente im Modell die jeweiligen auf den Anforderungen basierenden Constraints durch das Oracle geprüft (vgl. \cref{sec:requirements,subsec:yarnComponentConstraints,subsec:yarnController}).
Wenn ein Constraint nicht erfolgreich validiert wurde, wird dies jeweils im Programmlog gespeichert bzw. die Ausführung der Simulation abgebrochen, wenn sich das Cluster nicht mehr rekonfigurieren kann (vgl. \cref{subsec:oracleImpl}).

Nach Abschluss eines Testfalls werden durch den Simulator die in \cref{subsec:dataOrganisation} geforderten Daten im Programmlog abgespeichert, die beim Monitoring erkannt wurden.
Daneben werden bei der Ausführung eines Testfalls aber auch weitere Daten im Programmlog gespeichert wie \zB:

\begin{itemize}
    \item Ausführung von Komponentenfehlern
    \item Diagnostik"=Daten der \gls{YARN}"=Komponenten
    \item Verletzte Constraints und die betroffenen \gls{YARN}"=Komponenten
    \item Die Information, wenn eine Rekonfiguration nicht möglich ist
\end{itemize}

Zum Abschluss der Simulation werden zudem die für die Simulation als gesamtes betreffenden Constraints validiert, welche nicht im \gls{YARN}"=Modell selbst implementiert wurden (vgl. \cref{subsec:simulationBasics}).

Nach Abschluss der Simulation wird ein erneutes Monitoring des gesamten Clusters durchgeführt und der hierbei ermittelte Status als finaler Clusterstatus im Programmlog gespeichert.
Zudem werden einige statistische Kenndaten zum \gls{Test} im Programmlog gespeichert:

\begin{itemize}
    \item Gesamtdauer der Simulation
    \item Anzahl erfolgreicher Schritte
    \item Anzahl der maximal möglichen, aktivierbaren Komponentenfehler
    \item Anzahl aktivierter und deaktivierter Komponentenfehler
    \item Letzter ermittelter \gls{MARP}"=Wert
    \item Anzahl aller ausgeführten, erfolgreicher, fehlgeschlagenen sowie abgebrochener Anwendungen
    \item Anzahl aller ausgeführten Attempts
    \item Anzahl aller während der Ausführung erkannten Container
    \item Anzahl aller validierten Constraints und verletzten Constraints, getrennt nach \gls{SuT}- und Testsystem"=Constraints
\end{itemize}

Der auszugsweise Programmlog eines Tests findet sich in \cref{app:outputFormat}.